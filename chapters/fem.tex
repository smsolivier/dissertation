% !TEX root = ../doc.tex
\documentclass[../doc.tex]{subfiles}

\begin{document}
\chapter{Finite Element Preliminaries} \label{chap:fem}
The \gls{fem} is a popular technique for numerically approximating a wide variety of problems from mathematical physics. In FEM, partial differential equations are numerically solved using Galerkin's method where finite-dimensional functional spaces are used to approximate the weak, or variational, form of the problem.
The basic finite element method embellishes and extends Galerkin's method by defining a systematic process for building these approximation spaces such that they mimic the properties of the infinite-dimensional functional spaces associated with the continuous problem, lead to numerical solutions that increase in accuracy exponentially as the mesh is refined, and are amenable to efficient computer implementation. 

Finite element methods are characterized by three basis aspects. First, the domain is divided into a finite set of smaller domains called elements. The union of these elements forms a computational approximation of the domain, known as a mesh, that enables the simulation of problems defined on complex and irregular geometries. Second, the approximation spaces are formed in terms of a finite number of parameters, known as degrees of freedom, corresponding to piecewise polynomial functions. 
Suitable matching conditions are enforced so that these piecewise polynomial spaces are subsets of the desired infinite-dimensional function spaces. 
Finally, the approximation spaces are designed to be easily described using a canonical basis that has small support. Use of this canonical basis in Galerkin's method yields algebraic systems of equations that can be formed through computations associated with a single element and are sparse in that their entries are primarily zero. This means that, through a process known as finite element assembly, the entries of the algebraic system can be computed efficiently. In addition, the sparsity of the matrices allows use of specialized data structures that reduce the memory and computational costs of storing and solving the linear system.

This chapter introduces the finite element method and the required notation and machinery needed to understand and implement the finite element discretizations of the transport and moment systems discussed in subsequent chapters. The content of this chapter stems from a variety of sources. For mathematical analyses, \textcite{ciarlet1978finite} and \textcite{brenner_scott} are excellent for the standard conforming finite element method, \textcite{quateroni} and \textcite{mfem_boffi} for mixed finite elements, and \textcite{ern_guermond} for discontinuous finite elements. In addition, \textcite{zhodi} provides an introduction of the finite element method geared toward engineers. 

We begin with a high-level overview of weak forms and Galerkin's method. We present the element-local polynomial spaces used in this document to describe the mesh and solution spaces. The mesh is then defined paying particular attention to the construction of the reference to physical space transformations that allow integration over high-order elements along with the numerical integration of functions defined through these transformations. The finite element spaces used heavily in subsequent chapters are defined. The chapter concludes with implementation details associated with finite element assembly and an introduction to preconditioned iterative solvers. 

\section{The Weak Form and Galerkin's Method}
Finite element methods discretize and solve the weak, or variational, form of a partial differential equation. Consider the abstract problem: find $u \in X$ such that 
	\begin{equation} \label{fem:strong}
		A(u) = q \,,
	\end{equation}
where, for example, the operator $A$ could be the Poisson operator, 
	\begin{equation}
		A(u) = -\nabla^2u
	\end{equation}
with $q$ representing a source term and the solution space $X = \mathcal{C}^1$, the space of twice differentiable functions. The problem in Eq.~\ref{fem:strong} is referred to as the strong form and its solutions are often plagued by needlessly restrictive differentiability requirements. For example, consider the radiation diffusion equation given by
	\begin{equation} \label{fem:rad_strong}
		-\nabla\cdot D \nabla\varphi + \sigma_a \varphi = Q \,. 
	\end{equation}
Here, the diffusion coefficient, $D$, must be once differentiable and the solution, $\varphi$, must be twice differentiable. In problems with multiple materials, the diffusion coefficient can be discontinuous. Solving radiation diffusion in strong form then necessitates the use of domain decomposition methods where the problem is solved with a set of subproblems corresponding to each material that are coupled through interface conditions.  Furthermore, in many physical problems discontinuities in the solution, known as shocks, are possible. In such cases, the solution itself is not differentiable. In addition, the location of the discontinuities are not known a priori making the use of domain decomposition methods much more difficult. 

This motivates the use of reformulations of the strong form that relax, or weaken, the requirements on the differentiability of the solution and its coefficients. These weak forms are derived by multiplying the strong form by a suitably smooth (i.e.~differentiable) function and integrating over the domain. By applying integration by parts formulae, derivatives can be offloaded to the test function, leading to a weakening of the differentiability requirements. 

In this section, we provide insight into the use of test functions and weak derivatives, derive the weak form for the abstract problem $A(u) = q$ along with an example weak form for the radiation diffusion problem, discuss the mathematical spaces that are required to make the weak form well-defined, and define Galerkin's method for approximating the weak form of a partial differential equation numerically that underpins the finite element method. 

\subsection{Test Functions and Weak Derivatives}
Consider a one-dimensional function $f$ defined on the interval $[a,b]$. We ``test'' $f$ by multiplying it by an arbitrary function $v$ and integrating over the interval $[a,b]$. If, for example, $v = 1$ or $v=x$, we can glean information about the mean and variance of $f$, respectively. In addition, by setting the test function to the Dirac delta function centered at an arbitrary position $x$ we have that 
	\begin{equation}
		\int_a^b \delta(x' - x)\, f(x') \ud x' = f(x) \,, 
	\end{equation}
meaning integrating $f$ against a test function $v$ can be viewed as a generalization of inspecting a function with the familiar means of point-wise evaluation. Testing $f$ with $v$ is simply an alternative method for investigating the properties of $f$. 

Integrating against a test function is particularly useful for inspecting the properties of derivatives. Consider $f = \dderiv{g}{x}$ for some function $g$. Let $v$ be a differentiable function that satisfies $v = 0$ at $x = a$ and $x=b$. Testing $f$ with $v$ and integrating by parts yields 
	\begin{equation}
		\int_a^b v\dderiv{g}{x} \ud x = -\int_a^b \dderiv{v}{x} g \ud x \,. 
	\end{equation}
Thus, we can inspect the properties of $f = \dderiv{g}{x}$ without requiring that $g$ is differentiable! Leveraging these so-called ``weak derivatives'' is a key aspect of the success of the finite element method in modeling phenomena with discontinuous data or shocks in the solution. 

\subsection{An Abstract Weak Form}
For the abstract problem, multiplying by a test function $v$ and integrating over the domain yields 
	\begin{equation} \label{fem:weak_inter}
		\int v A(u) \ud \x = \int v\, q \ud \x \,.
	\end{equation}
Let the bilinear form $a(v,u)$ represent the operator derived by applying an integration by parts formula to $\int v\,A(u) \ud \x$ and the linear form $b(v) = \int v \, q \ud \x$. Furthermore, let $u,v \in V$ be a space of functions such that $a(v,u)$ is well defined. The statement $a(v,u) = b(v)$ is then an equivalent reformulation of Eq.~\ref{fem:weak_inter}. 
If we add a condition that this holds for all ($\equiv \forall$) test functions $v\in V$, then 
	\begin{equation} \label{fem:weak_abstract}
		a(v,u) = b(v) \,, \quad \forall v \in V\,,
	\end{equation}
implies that the strong form, $A(u) = q$, is satisfied provided that there exists a solution to the strong form. That is, when there exists $u \in X$ such that $A(u) = q$, the weak form in Eq.~\ref{fem:weak_abstract} is equivalent to the strong form. Note that if there does not exist a $u \in X$ that satisfies $A(u) = q$, there may still be a $u \in V$ that satisfies $a(v,u) = b(v)\,, \forall v \in V$. In general, we have that $X \subset V$ meaning the weak form allows a broader class of solutions. A sufficiently weak form defined over $V$ allows the solution of problems with discontinuous data or shocks in the solution that would not be possible using the strong form defined over $X$. 

In problems where it is possible, the solution space is typically restricted to functions that satisfy the boundary conditions. For example, if the problem contains the Dirichlet boundary condition $u = u^*$ for $\x \in \partial\D$, the solution space would be restricted to 
	\begin{equation}
		V_{u^*} = \{ u \in V : u|_{\partial\D} = u^* \} \,, 
	\end{equation}
the space of functions $u\in V$ that attain $u = u^*$ for $\x \in \partial\D$. 
This restricted solution space is supported by correspondingly restricting the test space to be zero on the boundary. That is, the test function $v \in V_0$. This modified weak form then reads: find $u \in V_{u^*}$ such that 
	\begin{equation}
		a(v,u) = b(v) \,, \quad \forall v \in V_0 \,. 
	\end{equation}

\subsection{Radiation Diffusion Example} \label{fem_sec:rad_diff}
Returning to the radiation diffusion example in Eq.~\ref{fem:rad_strong} with the additional requirement of a Dirichlet boundary condition $\varphi = 0$ on $\x\in\partial\D$, the weak form is: find $\varphi \in V$ such that 
	\begin{equation} \label{fem:rad_weak}
		\int \nabla u \cdot D \nabla\varphi \ud \x + \int \sigma_a\, u \varphi \ud \x = \int u\, Q \ud \x \,,\quad \forall u \in V \,. 
	\end{equation}
Here, $V = \{ u \in C^0 : u|_{\partial\D} = 0 \}$ is the space of continuous functions that are zero on the boundary of the domain. 
We have used the integration by parts formula: 
	\begin{equation}
		\int u\, \nabla\cdot\vec{v} \ud \x = \oint_{\partial\D} u\,\vec{v}\cdot\n \ud s - \int \nabla u \cdot \vec{v} \ud \x \,, 
	\end{equation}
along with the fact that $u = 0$ for $\x \in \partial\D$ since $u \in V$.  
Observe that the weak form in Eq.~\ref{fem:rad_weak} has reduced the differentiability requirements of both the solution and the diffusion coefficient. In particular, the test function and solution need only be once differentiable. In addition, there are no longer any differentiability requirements on the diffusion coefficient. 
Thus, the weak form expands the space of possible solutions from the space of twice differentiable functions to the space of once differentiable functions and allows discontinuous data. 

\subsection{Sobolev Spaces} \label{fem_sec:sobolev}
A key question is the choice of the space $V$ the weak form is defined over. For the strong form, the spaces $\mathcal{C}^i$ corresponding to functions having $i$ continuous derivatives for $i = 0,1,\ldots$ are a natural choice. However, the integrations used in the weak form require a more nuanced approach (see ``The Lebesgue integral'' from \textcite{reed1981functional}), the goal being to define the largest possible spaces such that the integrals in the weak form remain finite. That is, in the radiation diffusion example, we seek to define the space for $u$ and $\varphi$ such that 
	\begin{equation} \label{fem:want_finite}
		\int \nabla u \cdot D\nabla\varphi \ud \x < \infty \,, \quad \int \sigma_a\, u \varphi \ud\x < \infty \,, \quad \int u\,q \ud \x < \infty \,. 
	\end{equation}
Such properties are achieved by using Hilbertian Sobolev spaces. These spaces are used frequently throughout mathematical physics and are a natural choice for the bilinear and linear forms arising in finite element methods. Following standard notation, we define 
	\begin{equation}
		L^2(\D) = \{u : \D \rightarrow \R : \int_\D u^2 \ud \x < \infty \} \,, 
	\end{equation}
as the space of square-integrable functions. Note that this space does not place any requirements on the differentiability of its elements; functions in $L^2(\D)$ need only be square integrable. We will also use the space of functions with square-integrable gradient defined as: 
	\begin{equation}
		H^1(\D) = \{ u \in L^2(\D) : \int_\D \nabla u \cdot \nabla u \ud \x < \infty \} \,, 
	\end{equation}
and the space of vector-valued functions with square-integrable divergence: 
	\begin{equation}
		H(\div;\D) = \{ \vec{v} \in [L^2(\D)]^{\dim} : \nabla\cdot\vec{v} \in L^2(\D) \} \,. 
	\end{equation}
Square integrability is a desired property due to the following result. 
\begin{prop}
The product of two square-integrable functions is integrable. 
\end{prop}
\begin{proof}
We must show that given $u,v \in L^2(\D)$, $\int u v \ud \x < \infty$. Observe that 
	$$ (u + v)^2 = u^2 + 2 uv + v^2 \geq 0 \iff uv \leq \frac{1}{2}\paren{u^2 + v^2} \,. $$ 
Thus, 
	$$ \int uv \ud \x \leq \int \frac{1}{2}\paren{u^2 + v^2} \ud \x = \frac{1}{2}\int u^2 \ud \x + \frac{1}{2} \int v^2 \ud \x < \infty \,, $$
since $u,v \in L^2(\D)$ are square-integrable. 
\end{proof}
\noindent Thus, under mild assumptions on the data $D$, $\sigma_a$, and $q$, we have that
	\begin{subequations}
	\begin{equation}
		\int \sigma_a\, u \varphi \ud \x < \infty \,, \quad \forall u,\varphi \in L^2(\D) \,,
	\end{equation}
	\begin{equation}
		\int u\, q \ud \x < \infty \,, \quad \forall u \in L^2(\D) \,,
	\end{equation}
	\begin{equation}
		\int \nabla u \cdot D \nabla\varphi \ud \x < \infty \,, \quad \forall u, \varphi \in H^1(\D) \,. 
	\end{equation}
	\end{subequations}
Thus, since $H^1(\D) \subset L^2(\D)$, taking $u,\varphi \in H^1(\D)$ makes all terms in the radiation diffusion weak form well defined. In other words, the proper choice for the test and solution space is $V = H^1(\D)$. 

\subsection{Galerkin's Method}
We now construct a finite-dimensional approximation for the abstract problem in weak form given by: find $u \in V$ such that 
	\begin{equation} \label{fem:weak_gm}
		a(v,u) = b(v) \,, \quad \forall v \in V \,. 
	\end{equation}
The Lax-Milgram theorem (\textcite[\S 6.2.1]{evans2010partial}) states that Eq.~\ref{fem:weak_gm} has a unique solution when $a(\cdot,\cdot)$ is continuous, i.e., there exists $\alpha>0$ such that 
	\begin{equation}
		|a(v,u)| \leq \alpha \|u\|\|v\| \,, \quad \forall u,v \in V \,,
	\end{equation}
and coercive, i.e., there exists $\beta > 0$ such that 
	\begin{equation}
		a(u,u) \geq \beta \|u\|^2 \,, \quad \forall u \in V \,. 
	\end{equation}
Note that the condition of coercivity is a stronger form of positive definiteness. \textcite{la2fa} connects the constants $\alpha,\beta$ to the condition number of the bilinear form $a(\cdot,\cdot)$, denoted $\kappa(a)$, as 
	\begin{equation}
		\kappa(a) \leq \alpha/\beta \,. 
	\end{equation}
The condition number is commonly used as a proxy for the ``difficulty'' of solving a problem with iterative methods. The constants $\alpha$ and $\beta$ depend on the bilinear form $a$ and on the choice for the space $V$. Thus, the choice of $V$ impacts both the existence and uniqueness of the solution to Eq.~\ref{fem:weak_gm} as well as the ease with which it can be solved. 

Galerkin's method for approximating the problem in Eq.~\ref{fem:weak_gm} consists of defining similar problems in finite-dimensional spaces $V_h$. The conforming finite element method restricts the finite-dimensional approximation space to be $V_h \subset V$. To be specific, for any finite-dimensional subspace $V_h\subset V$ we define the discrete problem to be: find $u_h \in V_h$ such that 
	\begin{equation} \label{fem:galerkin}
	 	a(v_h,u_h) = b(v_h) \,, \quad \forall v_h \in V_h \,. 
	\end{equation} 
Since $V_h \subset V$, we can directly apply the Lax-Milgram theorem to show that the discrete problem in \ref{fem:galerkin} has a unique solution. Furthermore, we have that 
	\begin{equation}
		\kappa(a_h) \leq \alpha/\beta \,, 
	\end{equation}
where $a_h$ is $a$ restricted to the space $V_h$ \cite[Corollary 2.4]{la2fa}. Thus, by defining the approximation space as a subset of the infinite-dimensional space, the discrete operator inherits the analytic structure of the underlying weak problem. 
Note that in this document, we also consider non-conforming methods where the approximation space is not a subspace of $V$. In the case $V_h \not\subset V$, additional requirements are placed on the discrete problem in order to guarantee solvability and stability. This is an important aspect of the \gls{dg} methods presented in Chapter \ref{chap:dgvef}. 

Such a general framework as Galerkin's method, however, does not provide a way to define the approximation space $V_h$. The finite element method fills this gap by providing a systematic process for constructing the finite-dimensional spaces $V_h$ in ways that are amenable to efficient computer implementation. 

\section{Local Polynomial Spaces}
Both the mesh and the solution are represented with a polynomial space defined locally on each element. Let $\hat{K}^{\dim}$ denote the $\dim$-dimensional reference element which we set to the unit $\dim$-dimensional cube, $\hat{K} = [0,1]^{\dim}$. We omit the superscript denoting the dimensionality when the dimensionality of the reference element can be inferred from context. We define the space of univariate polynomials of degree less than or equal to $k$ as: 
	\begin{equation}
		\mathcal{P}_k(\hat{K}^1) = \{ p : \hat{K}^1 \rightarrow \R : p = \sum_{i=0}^{k} \alpha_i x^i \,, \alpha_i \in \R\ \forall\,i \in [0,k]\} = \spn\{ 1, x, x^2, \ldots, x^k \} \,. 
	\end{equation}
Multi-dimensional polynomial spaces are built through tensor products of the one-dimensional space. The tensor product polynomial spaces are:  
	\begin{equation}
		\mathcal{Q}_{m,n}(\hat{K}^2) = \{ p(x) q(y) : p \in \mathcal{P}_{m}(\hat{K}^1)\,, q \in \mathcal{P}_{n}(\hat{K}^1) \} 
	\end{equation}
in two dimensions and 
	\begin{equation}
		\mathcal{Q}_{\ell,m,n}(\hat{K}^3) = \{ p(x) q(y) r(z) : p \in \mathcal{P}_{\ell}(\hat{K}^1)\,, q \in \mathcal{P}_{m}(\hat{K}^1)\,, r \in \mathcal{P}_{n}(\hat{K}^1)\} 
	\end{equation}
in three dimensions. 
The tensor product polynomial space of equal degree in each variable is denoted by 
	\begin{equation}
		\Qcal{p} = \begin{cases}
			\Qcal{p,p} \,, & \dim = 2 \\ 
			\Qcal{p,p,p} \,, & \dim = 3 
		\end{cases} \,. 
	\end{equation}

Nodal bases for the space $\Pcal{p}$ are constructed using Lagrange interpolating polynomials. We consider interpolation through the Gauss-Lobatto and Gauss-Legendre points. Let $\{\xi_i\}$ represent the $p+1$ one-dimensional Gauss-Lobatto or Gauss-Legendre points in the interval $[0,1]$. Let $\ell_i$ denote the Lagrange interpolating polynomial satisfying $\ell_i(\xi_j) = \delta_{ij}$ where $\delta_{ij}$ is the Kronecker delta. The set of functions $\{\ell_i\}$ form a basis for $\Pcal{p}$. The basis functions can be written as:  
	\begin{equation}
		\ell_i(\xi) = \prod_{\substack{0\leq j \leq k\\i\neq j}} \frac{\xi - \xi_j}{\xi_i - \xi_j} \,, \quad i \in [0,k] \,. 
	\end{equation}
Alternatively, writing $\ell_i(\xi) = \sum_{j=0}^k c_{ij} \xi^j$, the coefficients $c_{ij}$ that interpolate through the $\xi_i$ can be found by solving the Vandermonde system 
	\begin{equation}
		\begin{bmatrix} 
			1 & \xi_0 & \xi_0^2 & \ldots & \xi_0^k \\
			1 & \xi_{1} & \xi_{1}^2 & \ldots & \xi_{1}^k \\
			\vdots & \vdots & \vdots & \ddots & \vdots \\
			1 & \xi_{k} & \xi_{k}^2 & \ldots & \xi_{k}^k 
		\end{bmatrix}
		\mat{C} = \I \,,
	\end{equation}
where $\mat{C} \in \R^{(k+1)\times(k+1)}$ is the matrix of coefficients with entries $c_{ij}$. The Vandermonde system is simply a change of basis from $\{1,x,x^2,\ldots\}$ to the nodal basis. Nodal bases for the spaces $\Qcal{m,n}$ and $\Qcal{\ell,m,n}$ are formed through tensor products of the corresponding one-dimensional nodal bases.
The basis functions for $\Pcal{1}$, $\Pcal{2}$, and $\Pcal{3}$ through the Gauss-Lobatto and Gauss-Legendre points are shown in Fig.~\ref{fem:shape1d} and \ref{fem:shape_leg1d}, respectively.  Figure \ref{fem:shape2d} shows a selection of the two-dimensional nodal basis functions through the Gauss-Lobatto points for the spaces $\Qcal{1}$, $\Qcal{2}$, and $\Qcal{3}$. 
% --- shape functions in 1D (Lobatto) --- 
\begin{figure}
\centering
\begin{subfigure}{.30\textwidth}
	\centering
	\includegraphics[width=\textwidth]{figs/shape1.pdf}
	\caption{}
\end{subfigure}
\begin{subfigure}{.30\textwidth}
	\centering
	\includegraphics[width=\textwidth]{figs/shape2.pdf}
	\caption{}
\end{subfigure}
\begin{subfigure}{.30\textwidth}
	\centering
	\includegraphics[width=\textwidth]{figs/shape3.pdf}
	\caption{}
\end{subfigure}
\caption{Plots of the one-dimensional shape functions through the Gauss-Lobatto nodes for (a) linear, (b) quadratic, and (c) cubic polynomial orders.}
\label{fem:shape1d}
\end{figure}

% --- shape functions in 1D (Legendre) --- 
\begin{figure}
\centering
\begin{subfigure}{.30\textwidth}
	\centering
	\includegraphics[width=\textwidth]{figs/shape_leg1.pdf}
	\caption{}
\end{subfigure}
\begin{subfigure}{.30\textwidth}
	\centering
	\includegraphics[width=\textwidth]{figs/shape_leg2.pdf}
	\caption{}
\end{subfigure}
\begin{subfigure}{.30\textwidth}
	\centering
	\includegraphics[width=\textwidth]{figs/shape_leg3.pdf}
	\caption{}
\end{subfigure}
\caption{Plots of the one-dimensional shape functions through the Gauss-Legendre nodes for (a) linear, (b) quadratic, and (c) cubic polynomial orders.}
\label{fem:shape_leg1d}
\end{figure}

% --- shape functions in 2D --- 
\begin{figure}
\centering
\includegraphics[width=.85\textwidth]{figs/shape2d.pdf}
\caption{Location of the interpolating points (upper) and a selection of nodal basis functions (lower) for the tensor product polynomial spaces $\Qcal{1}$, $\Qcal{2}$, and $\Qcal{3}$ in two dimensions.}
\label{fem:shape2d}
\end{figure}

Interpolation through the Gauss-Lobatto and Gauss-Legendre points both have the required properties to be accurate in the limit as $p\rightarrow \infty$. Thus, the choice of interpolating points is typically dictated by other aspects of the overall algorithm. Note that the Gauss-Lobatto points include the interval end points $0$ and $1$ while the Gauss-Legendre points do not. The bases resulting from Lagrange interpolation through the Gauss-Lobatto and Gauss-Legendre points are referred to as closed and open, respectively, due to this. The Gauss-Legendre basis has the beneficial property of diagonal mass matrices on affine meshes while the closed Gauss-Lobatto basis typically leads to sparser globally coupled systems since closed bases couple fewer degrees of freedom on interior faces. 

\section{Description of the Mesh}
\subsection{Admissible Tesselations}
% --- admissible meshes --- 
\begin{figure}
\centering
\begin{subfigure}{.45\textwidth}
	\centering
	\includegraphics[width=\textwidth]{figs/admissible.pdf}
	\caption{}
	\label{fem:admissible}
\end{subfigure}
\begin{subfigure}{.45\textwidth}
	\centering
	\includegraphics[width=\textwidth]{figs/inadmissible.pdf}
	\caption{}
	\label{fem:inadmissible}
\end{subfigure}
\caption{Examples of (a) an admissible tesselation and (b) an inadmissible tesselation.}
\end{figure}

Let $\D \subset \R^{\dim}$ with $\dim = 2,3$ be the domain of the problem. The domain is subdivided into a finite number of subsets $K$, called finite elements, such that 
	\begin{equation}
		\D = \bigcup_{K \in \meshT} K \,, 
	\end{equation}
where $\meshT$ denotes a tesselation of the domain called the mesh. 
In this document, we only consider the use of tensor product elements. That is, in two dimensions each $K$ is a quadrilateral and in three dimensions each $K$ is a hexahedron. The tesselation of the domain satisfies the following properties: 
	\begin{enumerate}
		\item each $K$ is a closed set with an interior, denoted $\mathring{K}$, that is non-empty, 
		\item the elements do not overlap i.e.~for each distinct $K_1,K_2 \in \meshT$, $\mathring{K}_1 \cap \mathring{K}_2 = \emptyset$,
		\item \label{fem:adjacent_elements}any face of $K_1 \in \meshT$ is either a subset of the boundary of the domain or a face of another $K_2 \in \meshT$.  
	\end{enumerate}
An example of an admissible mesh is shown in Fig.~\ref{fem:admissible}. The elements have non-empty interior, do not overlap, and all faces are either a subset of the boundary or a face of another element in the mesh. Figure \ref{fem:inadmissible} shows an inadmissible mesh that violates requirement \ref{fem:adjacent_elements} since there are faces in the mesh that correspond to only a subset of another element's boundary. Such a mesh is said to have ``hanging nodes'' which require a special treatment that we do not consider in this document. 

\subsection{Mathematical Notation}
We define $\Gamma$ as the set of unique faces in the mesh with $\Gamma_0 = \Gamma\setminus \partial\D$ the set of interior faces and $\Gamma_b = \Gamma \cap \partial\D$ the set of boundary faces so that $\Gamma = \Gamma_0 \cup \Gamma_b$. We denote the outward unit normal to element $K$ as $\n_K$. On an interior face $\mathcal{F} \in \Gamma_0$ between elements $K_1$ and $K_2$, we use the convention that $\n$ is the unit vector perpendicular to the shared face $K_1 \cap K_2$ pointing from $K_1$ to $K_2$ (see Fig.~\ref{fem:jump_avg_diag}). On such an interior face, the jump, $\jump{\cdot}$, and average, $\avg{\cdot}$, are defined as 
	\begin{equation} \label{fem:jump_avg}
		\jump{u} = u_1 - u_2 \,, \quad \avg{u} = \frac{1}{2}(u_1 + u_2) \,, \quad \mathrm{on} \ \mathcal{F} \in \Gamma_0 \,, 
	\end{equation}
where $u_i = u|_{K_i}$ with analogous definitions for vectors. Note that a continuous function $u$ satisfies $\jump{u} = 0$ on each interior face. 
On boundary faces, the jump and average are set to 
	\begin{equation} \label{fem:jump_avg_bdr}
		\jump{u} = u \,, \quad \avg{u} = u \,, \quad \mathrm{on} \ \mathcal{F} \in \Gamma_b \,,
	\end{equation}
and likewise for vector-valued functions on the boundary. A straightforward computation shows that 
	\begin{equation} \label{fem:jumps_avg_id}
		\sum_{K\in\meshT} \int_{\partial K} u\, \vec{v}\cdot\n_K \ud s = \int_\Gamma \jump{u} \avg{\vec{v}\cdot\n} \ud s + \int_{\Gamma_0} \avg{u}\jump{\vec{v}\cdot\n} \ud s \,. 
	\end{equation}
We refer to this as the ``jumps and averages identity.'' The restriction of the integration to the interior faces for the second term on the right side of Eq.~\ref{fem:jumps_avg_id} is used so that only one term contributes on the boundary of the mesh and is supported by the definition of the jump and average on the boundary of the domain. 

We define the ``broken'' gradient, denoted by $\nablah$, obtained by applying the gradient locally on each element. That is, 
	\begin{equation} \label{fem:broken_grad}
		(\nablah u)|_{K} = \nabla(u|_K) \,, \quad \forall K \in \meshT \,. 
	\end{equation}
This distinction is important for the piecewise polynomial spaces discussed in Section \ref{fem_sec:fes}. 
% --- jumps and averages --- 
\begin{figure}
\centering
\includegraphics[width=.65\textwidth]{figs/jump_avg.pdf}
\caption{A depiction of a discontinuous, piecewise quadratic solution across two quadrilateral elements. The normal vector, $\n$, is defined as pointing from $K_1$ to $K_2$ along the face between $K_1$ and $K_2$.}
\label{fem:jump_avg_diag}
\end{figure}

\subsection{Reference Transformations}
Each element $K$ is obtained as the image of the reference element $\hat{K}$ under an invertible, polynomial mapping $\T : \hat{K} \rightarrow K$ with $\T \in [\Qcal{m}]^{\dim}$. 
The mapping $\T$ is derived from a set of global control points and the element-local nodal basis for $\Qcal{m}$, denoted $\{\ell_i\}$. Figure \ref{fem:quadmesh} shows an example mesh where the control points labeled 2, 7, and 12 are shared so that the mesh coordinates are continuous across the interface between the two elements. 
On each element, the mapping is 
	\begin{equation}
		\x(\vec{\xi}) = \T(\vec{\xi}) = \sum_i \x_i \ell_i(\vec{\xi}) 
	\end{equation}
where $\x \in K$, $\vec{\xi}\in \hat{K}$, and the $\x_i$ are the control points corresponding to element $K$. Figure \ref{fem:eltrans} depicts the mesh transformation used for the left element of Fig.~\ref{fem:quadmesh}. 
% --- element transformation --- 
\begin{figure}
\centering
\begin{subfigure}{.4\textwidth}
	\centering
	\includegraphics[height=1.75in]{figs/quad_mesh.pdf}
	\caption{}
	\label{fem:quadmesh}
\end{subfigure}
\begin{subfigure}{.59\textwidth}
	\centering
	\includegraphics[height=1.75in]{figs/eltrans.pdf}
	\caption{}
	\label{fem:eltrans}
\end{subfigure}
\caption{Depictions of (a) the mesh control points in a quadratic quadrilateral mesh and (b) the reference transformation used to describe the left element of (a).}
\end{figure}

Let $\vec{\xi} \in \hat{K}$ denote the reference coordinates and $\x \in \D$ the physical coordinates such that $\x = \T(\vec{\xi})$. The Jacobian matrix of the mapping is 
	\begin{equation}
		\F = \pderiv{\T}{\vec{\xi}} \in \R^{\dim\times\dim} \,, 
	\end{equation}
with $J = |\F|$ its determinant. 
In two dimensions, with $\x = \vector{x & y}$ and $\vec{\xi} = \vector{\xi & \eta}$, the Jacobian matrix is written as: 
	\begin{equation}
		\F = \begin{bmatrix} 
			\pderiv{x}{\xi} & \pderiv{x}{\eta} \\ 
			\pderiv{y}{\xi} & \pderiv{y}{\eta} 
		\end{bmatrix} \,.
	\end{equation}
The partial derivatives of the mesh transformation are computed by taking derivatives of the nodal basis functions. In other words, 
	\begin{equation}
		\F = \sum_i \x_{i} \otimes \hnabla \ell_i \,,
	\end{equation}
where $\hnabla$ denotes the gradient with respect to $\vec{\xi}$. In two dimensions, this is equivalent to 
	\begin{equation}
		\mat{F} = \sum_{i} \begin{bmatrix} 
			x_{i} \pderiv{\ell_i}{\xi} & x_{i} \pderiv{\ell_i}{\eta} \\
			y_{i} \pderiv{\ell_i}{\xi} & y_{i} \pderiv{\ell_i}{\eta} 
		\end{bmatrix} \,, 
	\end{equation}
where $\x_{i} = \vector{x_{i} & y_{i}}$. 
The characteristic mesh length, $h$, is computed using the Jacobian of the transformation as: 
	\begin{equation}
		h = \max_{K\in\meshT} \paren{\int_{\hat{K}} J \ud\vec{\xi}}^{1/\dim} \,.
	\end{equation}

A mesh transformation is called affine when it can be written as
	\begin{equation}
		\T = \mat{A}\vec{\xi} + \vec{b}
	\end{equation}
where $\mat{A}\in\R^{\dim\times\dim}$ and $\vec{b}\in\R^{\dim}$ are constant with respect to $\vec{\xi}$. In such case, the Jacobian matrix is $\mat{F} = \mat{A}$ and the Hessian of the transformation, defined as $\frac{\partial^2 \x}{\partial^2 \vec{\xi}}$, is identically zero. Quadrilateral elements obtained by scaling, stretching along the $\xi$ or $\eta$ axes, or rotating the reference element are all affine while general quadrilateral elements, such as trapezoidal elements, and curved elements are not affine. Note that for affine elements, the inverse transformation, denoted $\T^{-1} : K \rightarrow \hat{K}$, is given by  
	\begin{equation}
		\T^{-1}(\x) = \mat{A}^{-1}(\x - \vec{b})
	\end{equation}
and is still polynomial. However, for non-affine elements, the inverse transformation is generally not polynomial. 

\subsection{Reference Transformations for Embedded Surfaces}
The faces $\mathcal{F} \in \Gamma$ are represented as $(\dim-1)$-dimensional surfaces embedded in the $\dim$-dimensional geometry. That is, a face in a two-dimensional problem is a one-dimensional line and a face in a three-dimensional problem is a two-dimensional plane. If $[\mathcal{Q}_m(\hat{K}^{\dim})]^{\dim}$ is used to define the volumetric reference transformation, the face is described by the polynomial $[\mathcal{Q}_m(\hat{K}^{\dim-1})]^{\dim-1}$ that interpolates through the nodal values corresponding to the face. 
Let $\x_i$ denote the control points corresponding to a single face in the mesh. For the face between the two elements depicted in the mesh shown in Fig.~\ref{fem:quadmesh}, the $\x_i$ would consist of the positions in two-dimensional space of the nodes labeled 2, 7, and 12. Letting $\{\ell_i\}$ denote the nodal basis for $\mathcal{Q}_m(\hat{K}^{\dim-1})$, the transformation for the face, denoted $\T_\mathcal{F} : \hat{K}^{\dim-1} \rightarrow \mathcal{F}$, is computed analogously to the volumetric transformation as: 
	\begin{equation}
		\T_\mathcal{F}(\vec{\xi}) = \sum_i \x_i \ell_i(\vec{\xi}) \,. 
	\end{equation}
However, for a face transformation, the referential coordinate, $\vec{\xi} \in \hat{K}^{\dim-1} \subset \R^{\dim-1}$, is of dimension one lower than that of the dimension of the mesh. Analogously to the volumetric transformation, the Jacobian matrix is: 
	\begin{equation}
		\F_\mathcal{F} = \pderiv{\T_\mathcal{F}}{\vec{\xi}} \in \R^{\dim \times (\dim - 1)} \,. 
	\end{equation}
Since $\mathcal{F}$ is an embedded surface, the Jacobian matrix is no longer square and thus $|\F_\mathcal{F}|$ is no longer well defined. To that end, we define $J_\mathcal{F}$ using the Gram determinant: 
	\begin{equation}
		J_\mathcal{F} = \sqrt{|\F^T \F|} \,. 
	\end{equation}
We then have that the characteristic length of the face is 
	\begin{equation}
		h_\mathcal{F} = \paren{\int_{\hat{K}^{\dim-1}} J_\mathcal{F}\ud \hat{s}}^{1/(\dim-1)} \,. 
	\end{equation}

The normal vector to $\mathcal{F}$ can be be computed by leveraging the fact that the gradient of the components of the embedded transformation points \emph{tangent} to the face. The normal is then the vector that is perpendicular to the tangent vectors. Note that since the Jacobian matrix of the transformation, $\F_\mathcal{F}$, represents the gradient of the transformation with respect to the reference coordinates, its columns represent tangent vectors. Since $\F_\mathcal{F} \in \R^{\dim \times (\dim-1)}$, the Jacobian matrix will have one tangent vector when $\dim = 2$ and two tangent vectors when $\dim = 3$. Let $\bvec{t}_i$ be the columns of $\F_\mathcal{F}$. We seek the vector $\n$ such that $\n \cdot \bvec{t}_i = 0$ for $1\leq i \leq \dim-1$. This can be achieved with a suitable cross product. In two dimensions, we use 
	\begin{equation}
		\n = \frac{\bvec{t}_1 \times \e_3}{\| \bvec{t}_1 \times \e_3 \|} 
	\end{equation}
so that the normal vector is a $90^\circ$ clockwise rotation of the tangent vector. In three dimensions, the normal is computed as the cross product of the two tangent vectors: 
	\begin{equation}
		\n = \frac{\bvec{t}_1 \times \bvec{t}_2}{\|\bvec{t}_1 \times \bvec{t}_2\|} \,. 
	\end{equation}
This ensures the normal will be perpendicular to both $\bvec{t}_1$ and $\bvec{t}_2$. This process is depicted in Fig.~\ref{fem:normal_diag} for the face between the two quadratic elements shown in Fig.~\ref{fem:quadmesh}. A three-dimensional example is shown in Fig.~\ref{fem:normal_diag3d}. Note that it is important to choose a consistent orientation of the embedded surface so that the normal always points outward. 
Due to its close relation to the gradient of the embedded transformation, the normal vector will vary as the gradient of the space used for the volumetric transformation. In other words, a mesh with linear faces will have constant normal vectors, quadratic faces will have linear normal vectors, etc. 
% --- quadratic line normal diagram --- 
\begin{figure}
\centering
\includegraphics[width=.5\textwidth]{figs/normal.pdf}
\caption{The tangent and normal vectors for the face between the two elements of the mesh depicted in Fig.~\ref{fem:quadmesh}. The transformation for the embedded surface between the two elements is defined by the quadratic polynomial in the space $\mathcal{Q}_2(\hat{K}^1)$ that interpolates between the nodes labeled 2, 7, and 12. The tangent vectors are computed from the Jacobian matrix of the embedded transformation. The normals are computed by rotating the tangent vector by $90^\circ$ in the clockwise direction.}
\label{fem:normal_diag}
\end{figure}
% --- 3D normal diagram --- 
\begin{figure}
\centering
\includegraphics[width=.5\textwidth]{figs/normal3d.pdf}
\caption{A depiction of a two-dimensional, linear quadrilateral element embedded in three dimensions. Here, the Jacobian matrix of the embedded transformation has two columns corresponding to the directions labeled $\bvec{t}_1$ and $\bvec{t}_2$. The normal vector points in the direction $\bvec{t}_1 \times \bvec{t}_2$, ensuring it is perpendicular to both $\bvec{t}_1$ and $\bvec{t}_2$. }
\label{fem:normal_diag3d}
\end{figure}

Alternatively, Nanson's formula \cite{ciarlet_elasticity,oden1971finite}: 
	\begin{equation}
		\n \ud s = J \F^{-T} \hat{\n} \ud \hat{s} 
	\end{equation}
can be used to compute the normal vector. Here, $\hat{\n}$ and $\ud \hat{s}$ are the normal and magnitude of the face in reference space, respectively. Note that the \emph{volumetric} Jacobian matrix and determinant are used. The normal vector is then 
	\begin{equation}
		\n = \frac{\F^{-T} \hat{\n}}{\| \F^{-T} \hat{\n}\|} \,. 
	\end{equation}
This formulation is advantageous when only the volumetric transformations are accessible. However, it requires knowledge of the location of the desired face in reference space (in order to find the reference coordinate to evaluate $J$ and $\F$ at) as well as the corresponding referential normal vector. 

\subsection{An Example Transformation: A Scaled Trapezoidal Element}
% --- trapezoidal transformation example --- 
\begin{figure}
\centering
\includegraphics[width=.85\textwidth]{figs/trap_trans.pdf}
\caption{An example of the mesh transformation associated with a linear quadrilateral element. }
\label{fem:trap_trans}
\end{figure}
Consider the linear, quadrilateral element depicted in Fig.~\ref{fem:trap_trans} where $\alpha,h\in \R$ with $h>0$. The case $\alpha = 0$ corresponds to a scaling of the reference element by $h$. Starting from the point $(0,0)$ in reference space and traversing the nodes counter-clockwise, the nodal basis for $\Qcal{1}$ is: 
	\begin{equation}
		\begin{array}{cc}
			\ell_1(\xi,\eta) = (1-\xi)(1-\eta)\,, & \ell_2(\xi,\eta) = \xi(1-\eta)\,,\\[2pt] \ell_3(\xi,\eta) = \xi\eta\,, & \ell_4(\xi,\eta) = (1-\xi)\eta\,. 
		\end{array} 
	\end{equation}
Let $\mat{N} = \vector{\ell_1 & \ldots & \ell_4}$ and $\mat{X}$ the matrix of nodal positions corresponding to the nodal basis such that 
	\begin{equation}
		\mat{X} = \begin{bmatrix} 
			0 & h & h + \alpha & -\alpha \\ 
			0 & 0 & h & h 
		\end{bmatrix} \,. 
	\end{equation}
With this notation, the transformation $\T = \sum_i \x_i \ell_i$ can be written as the matrix vector product: 
	\begin{equation}
		\T = \mat{X} \mat{N} \,. 
	\end{equation}
Simplifying terms yields: 
	\begin{equation}
		\T = \begin{bmatrix} 
			h \xi + \alpha \eta(2\xi - 1) \\ 
			h \eta 
		\end{bmatrix} \,, 
	\end{equation}
where $\vec{\xi} = \vector{\xi & \eta}$. Letting 
	\begin{equation}
		\mat{G} = \hnabla \mat{N} \in \R^{4\times 2} 
	\end{equation}
represent the matrix of gradients of each of the nodal basis functions, the Jacobian matrix can be computed with 
	\begin{equation}
		\F = \mat{X} \mat{G} \,, 
	\end{equation}
so that 
	\begin{equation}
		\F = \begin{bmatrix} \label{fem:Jacobian_example}
			2 \alpha \eta + h & \alpha(2\xi - 1) \\ 
			& h 
		\end{bmatrix} \,. 
	\end{equation}
Note that since $\F$ is not constant with respect to $\vec{\xi}$, this transformation is not affine. 
Taking the determinant of $\F$, we have that 
	\begin{equation}
		J = 2 \alpha \eta h + h^2 \,. 		
	\end{equation}
The area of the element can be computed in reference space with: 
	\begin{equation}
		\int_0^1 \int_0^1 J \ud \xi \ud \eta = h^2 + \alpha h \,. 
	\end{equation}
Observe that this formula matches geometrically computing the area of the scaled trapezoid as the sum of an $h\times h$ square and two triangles with base $\alpha$ and height $h$.  

The inverse transformation is found by solving $y = h\eta$ for $\eta$ and substituting this into the first coordinate of $\T$. The inverse transformation is then: 
	\begin{equation}
		\T^{-1}(\x) = \begin{bmatrix} 
			\frac{h x + \alpha y}{h^2 + 2\alpha y} \\ 
			y/h 
		\end{bmatrix} \,. 
	\end{equation}
Note that the inverse transformation is not polynomial in the first coordinate. 

We conclude with a computation of the embedded transformation associated with the right face of the transformation described by $\T$. This face corresponds to fixing $\xi = 1$ and letting $\eta \in [0,1]$ vary. Let 
	\begin{equation}
		b_1(\chi) = 1 - \chi \,, \quad b_2(\chi) = \chi \,,
	\end{equation}
be the nodal basis for $\mathcal{Q}_1([0,1])$ where $\chi \in [0,1]$ denotes the reference coordinate for $\mathcal{F}$. The embedded transformation is then
	\begin{equation}
		\T_\mathcal{F} = \begin{bmatrix} 
			h \\ 0 
		\end{bmatrix}(1-\chi) + 
		\begin{bmatrix} 
			h+\alpha \\ h
		\end{bmatrix} \chi = \begin{bmatrix} 
			h + \alpha \chi \\ 
			h\chi 
		\end{bmatrix} \,. 
	\end{equation}
The Jacobian is: 
	\begin{equation}
		\F = \dderiv{\T_\mathcal{F}}{\chi} = \begin{bmatrix} 
			\alpha \\ h 
		\end{bmatrix} \,. 
	\end{equation}
The Gram determinant is given by 
	\begin{equation}
		J_\mathcal{F} = \sqrt{|\F^T \F|} = \sqrt{\alpha^2 + h^2} \,. 
	\end{equation}
The length of the face is computed in reference space with 
	\begin{equation}
		\int_0^1 J_\mathcal{F}\ud \chi = \sqrt{\alpha^2 + h^2} 
	\end{equation}
which exactly matches the computation of the length using the distance formula applied to the points $(h,0)$ and $(h+\alpha,h)$. Finally, the normal vector to the face is computed by rotating the sole column of $\F$ by $-90^\circ$ and normalizing it: 
	\begin{equation}
		\n = \frac{\vector{h & -\alpha}}{\| \vector{h & -\alpha}\|} \,. 
	\end{equation}
On this linear face, the normal vector is constant. 

Alternatively, Nanson's formula can be used to compute the normal. The inverse transpose of the Jacobian for $\T$ from Eq.~\ref{fem:Jacobian_example} is 
	\begin{equation}
		\F^{-T} = \frac{1}{(2\alpha \eta + h)h} \begin{bmatrix} 
			h & 0 \\ 
			-\alpha(2\xi-1) & 2\alpha\eta+h
		\end{bmatrix} \,. 
	\end{equation}
The normal is computed by applying $\F^{-T}$ to the referential normal, $\hat{\n} = \vector{1&0}$, at a point $\vec{\xi}_\mathcal{F} = \vector{1 & \eta}$along the face and normalizing: 
	\begin{equation}
		\n = \frac{\F^{-T}|_{\vec{\xi}_\mathcal{F}}\hat{\n}}{\|\F^{-T}|_{\vec{\xi}_\mathcal{F}}\hat{\n}\|} = \frac{\vector{h & -\alpha}}{\|\vector{h & -\alpha}\|} \,. 
	\end{equation}

\section{Integration Transformations} \label{fem_sec:int_trans}
In this section, we present the machinery to transform integrands involving scalar and vector-valued functions and their derivatives between reference and physical space. These transformations allow integration over the arbitrary geometries defined by the map $\T$ using numerical quadrature rules defined on the reference element. These transformations and numerical quadrature are implicitly used to compute all of the bilinear and linear forms discussed in this document. 

\subsection{The Scalar Case}
For a scalar function $u : \D \rightarrow \R$, denote by $\hat{u} : \hat{K} \rightarrow \R$ its representation in reference space. The functions $u$ and $\hat{u}$ are related by 
	\begin{equation} \label{fem:scalar_trans}
		u(\x) = \hat{u}(\T^{-1}(\x)) \,.  	
	\end{equation}
Integration over the physical element is then equivalent to 
	\begin{equation}
		\int_K u \ud \x = \int_{\hat{K}} \hat{u}\, J\!\ud\vec{\xi} \,. 
	\end{equation}
The gradient of a scalar function transforms as
	\begin{equation} \label{fem:scalar_trans_grad}
		\nabla u = \mat{F}^{-T} \hnabla \hat{u} \,, 
	\end{equation}
where $\mat{F}^{-T}$ is the inverse transpose of the Jacobian matrix. Note that the inverse of the Jacobian matrix represents
	\begin{equation}
		\mat{F}^{-1} = \pderiv{\vec{\xi}}{\x} \,, 
	\end{equation}
and in two dimensions is given by 
	\begin{equation}
		\mat{F}^{-1} = \begin{bmatrix} 
			\pderiv{\xi}{x} & \pderiv{\xi}{y} \\ 
			\pderiv{\eta}{x} & \pderiv{\eta}{y} 
		\end{bmatrix} \,. 
	\end{equation}
The transformation of the gradient is derived using the chain rule. Observe that in two dimensions, 
	\begin{equation}
	\begin{aligned}
		\nabla \hat{u} &= \begin{bmatrix} 
			\pderiv{\hat{u}}{\xi} \pderiv{\xi}{x} + \pderiv{\hat{u}}{\eta}\pderiv{\eta}{x} \\[3pt]
			\pderiv{\hat{u}}{\xi} \pderiv{\xi}{y} + \pderiv{\hat{u}}{\eta}\pderiv{\eta}{y}
		\end{bmatrix} \\
		&= \underbrace{\begin{bmatrix} 
			\pderiv{\xi}{x} & \pderiv{\eta}{x} \\[3pt]
			\pderiv{\xi}{y} & \pderiv{\eta}{y} 
		\end{bmatrix}}_{\mat{F}^{-T}}
		\underbrace{
		\begin{bmatrix} 
			\pderiv{\hat{u}}{\xi} \\[3pt] \pderiv{\hat{u}}{\eta} 
		\end{bmatrix}}_{\hnabla \hat{u}} \,. 
	\end{aligned}
	\end{equation}
This derivation extends analogously to three dimensions. 

\subsection{The Vector Case}
Here, we present the transformations for vector-valued functions assuming $\dim = 2$ for the sake of brevity and note that the ideas presented here analogously extend to three dimensions. 
For vector-valued functions, the basis the vector is defined on must also be considered. The simplest basis is the canonical basis, $\e_i$, corresponding to the $x$ and $y$ axes. In this case, a vector $\vec{v} : \D \rightarrow \R^2$ is 
	\begin{equation} \label{fem:scalar_copies}
		\vec{v} = v_1 \e_1 + v_2 \e_2 
	\end{equation}
and each component transforms independently as $v_i = \hat{v}_i(\T^{-1}(\x))$. Writing
	\begin{equation}
		\nabla\vec{v} = \begin{bmatrix} 
			\pderiv{v_1}{x} & \pderiv{v_1}{y} \\ 
			\pderiv{v_2}{x} & \pderiv{v_2}{y}
		\end{bmatrix} = \begin{bmatrix} 
			(\nabla v_1)^T \\ 
			(\nabla v_2)^T 
		\end{bmatrix}		
	\end{equation}
then the gradient of a vector defined as in Eq.~\ref{fem:scalar_copies} transforms as 
	\begin{equation} \label{fem:scalar_copies_grad}
		\nabla\vec{v} = \begin{bmatrix} 
			(\mat{F}^{-T} \hnabla\hat{v}_1)^T \\
			(\mat{F}^{-T}\hnabla\hat{v}_2)^T 
		\end{bmatrix} \,. 
	\end{equation}
Note that defining a vector in this way does not preserve the normal or tangential components under a rotation. That is, $\vec{v}\cdot\n$ and $\vec{v}\cdot\tang$ are linear combinations of the $v_i$ instead of a single component representing the normal or tangential components, respectively. 

Alternatively, the contravariant Piola transform represents vectors on the so-called tangent basis so that the normal component can be preserved \cite{ciarlet_elasticity,piola_cisc}. Such a transformation is required by the Raviart Thomas space introduced in Section \ref{fem_sec:fes_rt} in order to strongly enforce continuity in the normal component of the current. The contravariant Piola transform is: 
	\begin{equation} \label{fem:piola}
		\vec{v} = \frac{1}{J}\mat{F}\hvec{v}\circ\T^{-1} \,. 
	\end{equation}
Here, $\hvec{v} : \hat{K} \rightarrow \R^2$ is a vector in reference space. 
Writing the columns of the Jacobian matrix as 
	\begin{equation}
		\mat{F} = \begin{bmatrix} 
			\bvec{t}_1 & \bvec{t}_2 
		\end{bmatrix} \,,
	\end{equation}
the contravariant Piola transformation is equivalent to
	\begin{equation}
		\vec{v} = \frac{1}{J}(\hat{v}_1 \bvec{t}_1 + \hat{v}_2 \bvec{t}_2) \,.
	\end{equation}
Observe that, on the reference canonical basis $\hat{\e}_i$, $\vec{v} = \hat{v}_1 \hat{\e}_1 + \hat{v}_2 \hat{\e}_2$, and thus the contravariant Piola transform maps the canonical reference basis to the tangent space spanned by $\{\bvec{t}_1,\bvec{t}_2\}$ and scales by $1/J$. 

When the mesh transformation $\T_e$ is not affine, the tangent basis is not orthogonal. In this case, the usual method of selecting components of a vector through the dot product (e.g.~$v_i = \bvec{t}_i \cdot \vec{v}$) is inappropriate since $\bvec{t}_i \cdot \bvec{t}_j \neq \delta_{ij}$. Instead, a dual basis, referred to as the cotangent basis, is constructed such that 
	\begin{equation}
		\bvec{n}_i \cdot \bvec{t}_j = \delta_{ij} \,. 
	\end{equation}
Since the $\bvec{t}_i$ are the columns of the Jacobian matrix, defining the cotangent basis as the rows of the inverse of the Jacobian matrix satisfies $\bvec{n}_i \cdot \bvec{t}_j = \delta_{ij}$ since $\mat{F}^{-1}\mat{F} = \I$. In other words, the cotangent basis is defined such that 
	\begin{equation}
		\mat{F}^{-1} = \begin{bmatrix} 
			\bvec{n}_1^T \\ \bvec{n}_2^T 
		\end{bmatrix} \,. 
	\end{equation}
For a contravariant vector, the usual method of selecting a component is now replaced with $v_i = \bvec{n}_i \cdot \vec{v}$. 
The cotangent space is associated with vectors normal to the faces. By representing the vector on the tangent space, the contravariant Piola transform allows selection of the component representing the normal component through $\n\cdot\vec{v}$. 
Note that for non-affine meshes, $\mat{F}$ depends on $\vec{\xi}$ and thus the tangent and cotangent bases also depend on $\vec{\xi}$. 

Figure \ref{fem:piola_diag} depicts an example non-affine mesh transformation and the tangent and cotangent bases evaluated at the point $\vec{\xi} = (0,0)$. Observe that the pairs $(\bvec{t}_1, \bvec{n}_2)$ and $(\bvec{t}_2, \bvec{n}_1)$ are perpendicular. The pairs $(\bvec{t}_1, \bvec{n}_1)$ and $(\bvec{t}_2, \bvec{n}_2)$ do not point in the same direction but their magnitudes and directions balance so that $\bvec{t}_i\cdot \bvec{n}_i = 1$. Thus, the bi-orthogonality condition $\bvec{n}_i\cdot\bvec{t}_j = \delta_{ij}$ is satisfied. In addition, the tangent vectors and cotangent vectors are tangential and normal, respectively, to one of the faces connecting at the point $\vec{\xi} = (0,0)$. 
% --- piola transform --- 
\begin{figure}
	\centering
	\includegraphics[width=.85\textwidth]{figs/piola.pdf}
	\caption{A depiction of the tangent and cotangent bases at the point $\vec{\xi} = (0,0)$ under a non-affine mesh transformation. }
	\label{fem:piola_diag}
\end{figure}

For a contravariant vector, 
	\begin{equation} \label{fem:piola_ibp1}
		\int_K \nabla u \cdot \vec{v} \ud \x = \int_{\hat{K}} \mat{F}^{-T}\hnabla \hat{u} \cdot \frac{1}{J}\mat{F} \hat{v}\, J\!\ud\vec{\xi} = \int_{\hat{K}} \hnabla \hat{u} \cdot \hvec{v} \ud\vec{\xi} \,. 
	\end{equation}
The gradient transforms as 
	\begin{equation} \label{fem:piola_grad}
		\nabla\vec{v} = \nabla\paren{\frac{1}{J}\mat{F}\hat{v}\circ\mat{T}^{-1}} = \frac{1}{J}\mat{F}\!\paren{\hnabla\hvec{v} - \hat{\mat{B}}}\!\mat{F}^{-1} 
	\end{equation}
where 
	\begin{equation}
		\hat{\mat{B}} = \frac{1}{J}\hnabla\!\paren{J\mat{F}^{-1}}\!\mat{F}\hvec{v} \,. 
	\end{equation}
This result is derived by direct computation in Section \ref{fem_sec:grad_piola} along with the details required to implement this transformation using the machinery commonly provided in finite element libraries. It is also shown that $\hmat{B} = 0$ when the mesh transformation is affine and that $\tr(\hat{\mat{B}}) = 0$. This last result is known as the Piola identity \cite{ciarlet_elasticity}. Using the Piola identity, the linearity of the trace, and the invariance of the trace under similarity transformations, the divergence transforms as
	\begin{equation} \label{fem:piola_div}
		\nabla\cdot\vec{v} = \tr\paren{\nabla\vec{v}} = \frac{1}{J}\tr\!\paren{\mat{F}\!\paren{\hnabla\hvec{v} - \hat{\mat{B}}}\!\mat{F}^{-1}} = \frac{1}{J}\hnabla\cdot\hvec{v} \,. 
	\end{equation}
Thus, 
	\begin{equation} \label{fem:piola_ibp2}
		\int_K u\, \nabla\cdot\vec{v} \ud \x = \int_{\hat{K}} \hat{u}\, \hnabla\cdot\hvec{v} \ud\vec{\xi} \,. 
	\end{equation}
Combining the results from Eqs.~\ref{fem:piola_ibp1} and \ref{fem:piola_ibp2} yields: 
	\begin{equation}
		\int_{\partial K} u\,\vec{v}\cdot\n \ud s = \int_{\partial\hat{K}} \hat{u}\, \hvec{v}\cdot\hat{\n} \ud \hat{s} \,, 
	\end{equation}
where $\hat{\n}$ is the normal vector in reference space corresponding to the physical space normal $\n$. 
In other words, the contravariant Piola transformation preserves the normal component. 

\subsection{The Gradient of the Piola Transformation} \label{fem_sec:grad_piola}
Here we derive a formula for the transformation of the gradient of a vector defined under the contravariant Piola transformation. For the contravariant Piola transform $\vec{v} = \frac{1}{J}\mat{F}\hvec{v} \circ \T^{-1}$ the inverse transform is: 
	\begin{equation}
		\hvec{v} = J\mat{F}^{-1}\vec{v} \circ \T \,. 
	\end{equation}
Here, we seek to derive 
	\begin{equation}
		\hnabla\hvec{v} = \hnabla\!\paren{J\mat{F}^{-1}\vec{v}} \,, 
	\end{equation}
so that we can solve for $\nabla\vec{v}$. The goal is to derive the functional form of the transformation in terms of functionality commonly implemented in finite element codes. That is, we cast the computation in terms of the Jacobian matrix and Hessian of the transformation. 

Through their connection to the Jacobian matrix and the inverse of the Jacobian matrix, the tangent and cotangent spaces are related by 
	\begin{equation}
		\bvec{n}_1 = \bvec{t}_2 \times \hat{\e}_3 \,, \quad \bvec{n}_2 = \hat{\e}_3 \times \bvec{t}_1 \,,
	\end{equation}
where $\hat{\e}_3$ points out of the page. In other words, $\bvec{n}_1$ is a 90 degree clockwise rotation of $\bvec{t}_2$ and $\bvec{n}_2$ is a 90 degree counterclockwise rotation of $\bvec{t}_1$ (see Fig.~\ref{fem:piola_diag}). Thus, we can write 
	\begin{equation}
	\begin{aligned}
		\hnabla\hvec{v} &= \hnabla \begin{bmatrix} 
			J \bvec{n}_1\cdot\vec{v} \\ J \bvec{n}_2 \cdot \vec{v} 
		\end{bmatrix}\\
		&= \begin{bmatrix} 
			\pderiv{}{\xi}(J\bvec{n}_1\cdot\vec{v}) & \pderiv{}{\eta}(J\bvec{n}_1\cdot\vec{v}) \\ 
			\pderiv{}{\xi}(J\bvec{n}_2\cdot\vec{v}) & \pderiv{}{\eta}(J\bvec{n}_2 \cdot \vec{v}) 
		\end{bmatrix} \\
		&= \begin{bmatrix} 
			\pderiv{}{\xi}(J\bvec{n}_1)\cdot\vec{v} & \pderiv{}{\eta}(J\bvec{n}_1)\cdot\vec{v} \\ 
			\pderiv{}{\xi}(J\bvec{n}_2)\cdot\vec{v} & \pderiv{}{\eta}(J\bvec{n}_2)\cdot\vec{v} 
		\end{bmatrix}
		+ \begin{bmatrix} 
			J\bvec{n}_1 \cdot\pderiv{\vec{v}}{\xi} & J\bvec{n}_1 \cdot\pderiv{\vec{v}}{\eta}\\
			J\bvec{n}_2 \cdot\pderiv{\vec{v}}{\xi} & J\bvec{n}_2 \cdot\pderiv{\vec{v}}{\eta}
		\end{bmatrix} \,.
	\end{aligned}
	\end{equation}
The second term can be written as 
	\begin{equation}
		\begin{bmatrix} 
			J\bvec{n}_1 \cdot\pderiv{\vec{v}}{\xi} & J\bvec{n}_1 \cdot\pderiv{\vec{v}}{\eta}\\
			J\bvec{n}_2 \cdot\pderiv{\vec{v}}{\xi} & J\bvec{n}_2 \cdot\pderiv{\vec{v}}{\eta}
		\end{bmatrix}
		= J\mat{F}^{-1}\hnabla\vec{v} = J\mat{F}^{-1}\nabla\vec{v}\mat{F} \,, 
	\end{equation}
where $\hnabla\vec{v} = \nabla\vec{v}\mat{F}$ transforms the reference gradient to the physical gradient. The first term is a third-order tensor contracted with a vector to yield a second-order tensor. By expanding the dot products, we can emulate this contraction as a sum of two second-order tensors: 
	\begin{equation}
	\begin{aligned}
		\begin{bmatrix} 
			\pderiv{}{\xi}(J\bvec{n}_1) \cdot \vec{v} & \pderiv{}{\eta}(J\bvec{n}_1)\cdot\vec{v} \\ 
			\pderiv{}{\xi}(J\bvec{n}_2)\cdot\vec{v} & \pderiv{}{\eta}(J\bvec{n}_2)\cdot\vec{v} 
		\end{bmatrix} &= 
		\begin{bmatrix} 
			\pderiv{}{\xi}(Jn_{11})v_1 + \pderiv{}{\xi}(Jn_{12})v_2 & \pderiv{}{\eta}(Jn_{11})v_1 + \pderiv{}{\eta}(Jn_{12})v_2 \\
			\pderiv{}{\xi}(Jn_{21})v_1 + \pderiv{}{\xi}(Jn_{22})v_2 & \pderiv{}{\eta}(Jn_{21})v_1 + \pderiv{}{\eta}(Jn_{22})v_2 
		\end{bmatrix} \\
		&= \begin{bmatrix} 
			\pderiv{}{\xi}(Jn_{11}) & \pderiv{}{\eta}(Jn_{11}) \\ 
			\pderiv{}{\xi}(Jn_{21}) & \pderiv{}{\eta}(Jn_{21}) 
		\end{bmatrix} v_1 + 
		\begin{bmatrix} 
			\pderiv{}{\xi}(Jn_{12}) & \pderiv{}{\eta}(Jn_{12}) \\ 
			\pderiv{}{\xi}(Jn_{22}) & \pderiv{}{\eta}(Jn_{22}) 
		\end{bmatrix} v_2 \\
		&= \hnabla (J\F_1^{-1}) v_1 + \hnabla(J \F_2^{-1}) v_2
	\end{aligned}
	\end{equation}
where $\F^{-1}_i$ are the columns of $\F^{-1}$. Typically, finite element codes provide the Hessian matrix of the forward map but not the inverse map. Thus, to leverage existing functionality, we must write the above matrices in terms of $\H = \hnabla\mat{F}$ instead of $\hnabla\mat{F}^{-1}$. Assume that the code computes the Hessian matrix in \emph{flattened} and symmetric form as:  
	\begin{equation}
		\ang{\H} = \begin{bmatrix} 
			\frac{\partial^2 x}{\partial \xi^2} & \frac{\partial^2 x}{\partial\xi\partial\eta} & \frac{\partial^2 x}{\partial\eta^2} \\
			\frac{\partial^2 y}{\partial \xi^2} & \frac{\partial^2 y}{\partial\xi\partial\eta} & \frac{\partial^2 y}{\partial\eta^2}
		\end{bmatrix} \,. 
	\end{equation}
Then the above can be rewritten as 
	\begin{equation}
	\begin{aligned}
		\hnabla(J\F_1^{-1}) &= \hnabla \begin{bmatrix} 
			F_{22} \\ -F_{21} 
		\end{bmatrix} \\
		&= \hnabla\begin{bmatrix} 
			\pderiv{y}{\eta} \\ -\pderiv{y}{\xi} 
		\end{bmatrix}\\
		&= \begin{bmatrix} 
			\frac{\partial^2 y}{\partial\xi\partial\eta} & \frac{\partial^2 y}{\partial\eta^2} \\ 
			-\frac{\partial^2 y}{\partial\xi^2} & -\frac{\partial^2 y}{\partial\xi\partial\eta} 
		\end{bmatrix}\\
		&= \begin{bmatrix} 
			H_{22} & H_{23} \\ -H_{21} & -H_{22}
		\end{bmatrix} \,, 
	\end{aligned}
	\end{equation}
	\begin{equation}
	\begin{aligned}
		\hnabla(J\F_2^{-1}) &= \hnabla \begin{bmatrix} 
			-F_{12} \\ F_{11} 
		\end{bmatrix} \\
		&= \hnabla\begin{bmatrix} 
			-\pderiv{x}{\eta} \\ \pderiv{x}{\xi} 
		\end{bmatrix}\\
		&= \begin{bmatrix} 
			-\frac{\partial^2 x}{\partial\xi\partial\eta} & -\frac{\partial^2 x}{\partial\eta^2} \\ 
			\frac{\partial^2 x}{\partial\xi^2} & \frac{\partial^2 x}{\partial\xi\partial\eta} 
		\end{bmatrix}\\
		&= \begin{bmatrix} 
			-H_{12} & -H_{13} \\ H_{11} & H_{12} 
		\end{bmatrix} \,. 
	\end{aligned}
	\end{equation}
We can define the matrix 
	\begin{equation} \label{fem:bmat_parts}
		\hat{\mat{B}} = \hnabla(J\mat{F}^{-1}) \vec{v} = \begin{bmatrix} 
			H_{22} & H_{23} \\ -H_{21} & -H_{22}
		\end{bmatrix} v_1 + 
		\begin{bmatrix} 
			-H_{12} & -H_{13} \\ H_{11} & H_{12} 
		\end{bmatrix} v_2 \,. 
	\end{equation}
This is computed in flattened form as 
	\begin{equation} \label{fem:bhat_flat}
	\begin{aligned}
		\ang{\hat{\mat{B}}} &= \begin{bmatrix} 
			\ang{\hnabla(J\F_1^{-1})} & \ang{\hnabla(J\F_2^{-1})}
		\end{bmatrix} \vec{v} \\
		&= \begin{bmatrix} 
			H_{22} & -H_{12} \\ H_{23} & -H_{13} \\ -H_{21} & H_{11} \\ -H_{22} & H_{12} 
		\end{bmatrix} \frac{1}{J}\F\hvec{v} 
	\end{aligned}
	\end{equation}
where $\vec{v} = \frac{1}{J}\F\hvec{v}$ was used. Finally, we have that 
	\begin{equation}
		\hnabla \hvec{v} = \hat{\mat{B}} + J \F^{-1} \nabla\vec{v} \F \iff \nabla\vec{v} = \frac{1}{J}\F\!\paren{\hnabla\hvec{v} - \hat{\mat{B}}}\!\F^{-1} \,. 
	\end{equation}
We can then say that 
	\begin{equation} \label{fem:nablaE_trans}
	\begin{aligned}
		\nabla\vec{v} : \E \ud \x &= \frac{1}{J}\F\!\paren{\hnabla\hvec{v} - \hat{\mat{B}}}\!\F^{-1} : \E \, J \!\ud \vec{\xi} \\
		&= \paren{\hnabla\hvec{v} - \hat{\mat{B}}} : \F^{T} \E \F^{-T} \,\ud\vec{\xi} \,. 
	\end{aligned}
	\end{equation}
Here, we use the fact that $\mat{A} : \mat{B} = \tr(\mat{A}\mat{B}^T)$ and apply the cyclic property of the trace to permute $\mat{F}$ and $\mat{F}^{-1}$. 
In this form, we can implement the gradient calculation as a matrix-vector product of the flattened referential gradient and the coefficients of $\hvec{v}$. 

When the mesh transformation is affine, $\hmat{B} = 0$ since the Hessian of an affine transformation is zero. In addition, the Piola identity states that $\tr\hmat{B} = 0$. This can be most easily seen in Eq.~\ref{fem:bmat_parts} where 
	\begin{equation}
		\tr\hmat{B} = (H_{22} - H_{22}) v_1 + (-H_{12} + H_{12}) v_2 = 0 \,. 
	\end{equation}
Using the Piola identity and Eq.~\ref{fem:nablaE_trans}, we have that 
	\begin{equation}
	\begin{aligned}
		\nabla\cdot\vec{v} \ud \x &= \nabla\vec{v} : \I \ud \x \\
		&= \paren{\hnabla\hvec{v} - \hmat{B}} : \mat{F}^T\I\mat{F}^{-T} \ud \vec{\xi} \\
		&= \tr\paren{\hnabla\hvec{v} - \hmat{B}} \ud \vec{\xi} \\
		&= \hnabla\cdot\hvec{v} \ud \vec{\xi} \,. 
	\end{aligned}
	\end{equation}
Thus, in the thick diffusion limit when $\E \propto \I$, $\nabla\vec{v} : \E$ simplifies to the standard transformation for the divergence of a contravariant vector.

\subsection{Integration on Embedded Surfaces}
We now discuss the machinery needed to integrate a function defined on the mesh over an embedded surface in the mesh. The primary difficulty is that the domain of integration is described by a $\dim-1$-dimensional embedded surface while the integrand is defined as a grid function on the $\dim$-dimensional mesh. We thus need a way to convert from a reference point $\vec{\xi}_\mathcal{F} \in \hat{K}^{\dim-1}$ to a reference point $\vec{\xi}_e \in \hat{K}^{\dim}$ that corresponds to an adjacent element $K_e$. This is achieved through \emph{integration point transformations}, $\hat{\T}_e : \hat{K}^{\dim-1} \rightarrow \hat{K}^{\dim}$, which map the reference point for the embedded surface to a reference coordinate in an adjacent element $K_e$ such that 
	\begin{equation}
		\T_\mathcal{F}(\vec{\xi}_\mathcal{F}) = \T_e(\hat{\T}_e(\vec{\xi}_\mathcal{F})) \,. 
	\end{equation}
In other words, the points $\vec{\xi}_\mathcal{F} \in \hat{K}^{\dim-1}$ and $\vec{\xi}_e = \hat{\T}_e(\vec{\xi}_\mathcal{F}) \in \hat{K}^{\dim}$ correspond to the same point in physical space under the action of their associated transformations. An example of an integration point transformation that converts a point on $\hat{K}^{2}$ to a point on the right face of $\hat{K}^3$ is shown in Fig.~\ref{fem:embed_diag}. In this case, points $\vec{\xi}_\mathcal{F} = \vector{\xi_\mathcal{F} & \eta_\mathcal{F}}$ are mapped to the plane $\xi=1$ such that $\vec{\xi} = \vector{1 & \xi_\mathcal{F} & \eta_\mathcal{F}}$. 
% --- 3D embedded transformation --- 
\begin{figure}
\centering
\includegraphics[width=.85\textwidth]{figs/embed.pdf}
\caption{An example of an integration point transformation that maps $\hat{K}^2$ to the right face of $\hat{K}^3$. A point $\vec{\xi}_\mathcal{F} = \vector{\xi_\mathcal{F} & \eta_\mathcal{F}}$ is mapped to the plane $\xi=1$ of the three-dimensional reference element such that $\vec{\xi} = \vector{1 & \xi_\mathcal{F} & \eta_\mathcal{F}}$. The points $\vec{\xi}_\mathcal{F}$ and $\vec{\xi} = \hat{\T}(\vec{\xi}_\mathcal{F})$ satisfy $\T_\mathcal{F}(\vec{\xi}_\mathcal{F}) = \T(\vec{\xi})$ where $\T_\mathcal{F}$ and $\T$ are the reference to physical space transformations for the embedded face $\mathcal{F}$ and the volumetric element $K$, respectively. }
\label{fem:embed_diag}
\end{figure}

If $\mathcal{F} = K_1 \cap K_2$ and $f(u_1, u_2)$ is a function that depends on the functions $u_e : K_e \rightarrow \R$ defined on the elements that share the face $\mathcal{F}$, the integral  
	\begin{equation}
		\int_\mathcal{F} f(u_1, u_2) \ud s 
	\end{equation}
can be computed in reference space using 
	\begin{equation}
		\int_{\hat{K}^{\dim-1}} f(\bar{u}_1, \bar{u}_2)\, J_\mathcal{F}\!\ud \vec{\xi}_\mathcal{F} \,, 
	\end{equation}
where $\bar{u}_e(\vec{\xi}_\mathcal{F}) = u_e(\T_e(\hat{\T}_e(\vec{\xi}_\mathcal{F})))$ is a function of the reference coordinate of the $\dim-1$-dimensional face through the composition with the integration point transformation, $\hat{\T}_e$, and the volumetric transformation, $\T_e$, associated with element $K_e$. In this way, any function can be evaluated using the $\dim$-dimensional grid function by first transforming the $\dim-1$-dimensional location $\vec{\xi}_\mathcal{F}$ to the $\dim$-dimensional point associated with an adjacent element $K_e$ through $\vec{\xi}_e = \hat{\T}_e(\vec{\xi}_\mathcal{F})$. Note that if $f = f(u_1, u_2, \nabla u_1, \nabla u_2)$ the volumetric transformations discussed in this section can be applied to compute $\nabla u_1$ and $\nabla u_2$ by first converting the reference point on the face to a reference point on the volume. 

\section{Finite Element Spaces} \label{fem_sec:fes}
Finite element spaces are defined on the mesh $\meshT$ or the interior skeleton of the mesh $\Gamma_0$ and consist of an element-local function space and a set of inter-element matching conditions. 
The inter-element matching conditions enforce various types of continuity of the solution between elements. The combination of a locally smooth function space and suitable matching conditions allows finite element spaces to be discrete subspaces of Sobolev spaces such as $L^2(\D)$, $H^1(\D)$, and $H(\div;\D)$. The following subsections define the element-local function spaces and matching conditions used to discretize the transport and moment equations in subsequent chapters. 

\subsection{Discontinuous Galerkin} \label{fem_sec:dg}
The \gls{dg} space is a discrete subspace of $L^2(\D)$, the space of square-integrable functions. In other words, if $u$ is an element of the DG space, 
	\begin{equation}
		\int u^2 \ud \x < \infty \,. 
	\end{equation}
Since only square integrability is required, functions in $L^2(\D)$, and thus DG spaces, do not need to be continuous. DG functions are represented using piecewise-discontinuous polynomials that are defined on the reference element and mapped to the physical element using the inverse mesh transformation $\T^{-1}: K \rightarrow \hat{K}$. In other words, on each element, the solution belongs to: 
	\begin{equation}
		\Qbb{p}(K) = \{ u = \hat{u} \circ \T^{-1} : \hat{u} \in \mathcal{Q}_p(\hat{K}) \}\,. 
	\end{equation}
The distinction between $\Qcal{p}$ and $\Qbb{p}(K)$ is important for non-affine mesh transformations. In such case, the inverse mesh transformation is generally non-polynomial so that the composition $u = \hat{u}\circ\T^{-1}$ is also non-polynomial. 

The degree-$p$ DG space is 
	\begin{equation}
		Y_p = \{ u \in L^2(\D) : u|_{K} \in \Qbb{p}(K) \,, \quad \forall K \in \meshT \} \,. 
	\end{equation}
An example of the distribution of the degrees of freedom in a linear DG space on a $3\times 3$ mesh is shown in Fig.~\ref{fem:dgfes}. Note that degrees of freedom are not shared between elements. Since there are no continuity requirements in the DG space, the basis for the local polynomials can use either open or closed points. That is, a nodal basis can be formed with Lagrange interpolating polynomials through the $\dim$-fold Cartesian product of either the closed Gauss-Lobatto points or the open Gauss-Legendre points. 
% --- DG FES --- 
\begin{figure}
\centering
\includegraphics[width=.3\textwidth]{figs/dgfes.pdf}
\caption{A depiction of the distribution of degrees of freedom in the linear DG space. The Legendre nodes are used to illustrate that degrees of freedom are not shared between elements. }
\label{fem:dgfes}
\end{figure}

We additionally define the vector-valued DG space 
	\begin{equation}
		\vDG = \{ \vec{v} \in [L^2(\D)]^{\dim} : \vec{v}_i \in Y_p \,, \quad 1 \leq i \leq \dim \} \,. 
	\end{equation}
This space uses the scalar DG space for each component. 

\subsection{Continuous Finite Element}
Let the degree-$p$, scalar continuous finite element space be
	\begin{equation}
		V_p = \{ u \in C_0(\D) : u|_{K_e} \in \Qbb{p}(K_e) \,, \quad \forall K_e \in\meshT \} 
	\end{equation}
so that each function $u\in V_p$ is a piecewise-continuous polynomial mapped from the reference element. Since $u \in V_p$ is locally smooth and $V_p \subset C_0(\D)$, it can be shown that given $u\in V_p$, $\nabla u = \nablah u \in [L^2(\D)]^2$. Thus, $V_p \subset H^1(\D)$. The distribution of degrees of freedom for the space $V_2$ is shown in Fig.~\ref{fem:h1fes}. Here, continuity is enforced by sharing degrees of freedom between elements. Due to this, a nodal basis using closed points, such as the Gauss-Lobatto points, must be used. 
% --- CFEM FES ---
\begin{figure}
\centering
\includegraphics[width=.3\textwidth]{figs/h1fes.pdf}
\caption{A depiction of the distribution of degrees of freedom for the quadratic continuous finite element space. Continuity of members of the finite element space is enforced by sharing degrees of freedom across neighboring elements.}
\label{fem:h1fes}
\end{figure}

The vector-valued analog 
	\begin{equation}
		W_p = \{ \vec{v} \in [H^1(\D)]^{\dim}: v_i \in V_p \,, \quad 1 \leq i \leq \dim \} 
	\end{equation}
uses the scalar continuous finite element space for each component. In this way, $\vec{v} \in W_p \subset [H^1(\D)]^{\dim}$ and thus $\nabla\vec{v} = \nablah\vec{v} \in [L^2(\D)]^{\dim\times\dim}$. Since each component is defined independently using the scalar space, vectors $\vec{v} \in W_p$ transform according to Eq.~\ref{fem:scalar_copies}. 

\subsection{Raviart Thomas} \label{fem_sec:fes_rt}
The \gls{rt} space is a discrete subspace of $H(\div;\D)$, the space of vector-valued functions with square-integrable divergence. That is, 
	\begin{equation}
		H(\div;\D) = \{ \vec{v} \in [L^2(\D)]^2 : \nabla\cdot\vec{v} \in L^2(\D) \} \,. 
	\end{equation}
The requirements of a discrete subspace are codified in the following proposition. 
\begin{prop}[see \textcite{quateroni} Prop. 3.2.2] \label{fem:div_prop}
Let $\vec{v} : \D \rightarrow \R^2$ be such that 
\begin{enumerate}
	\item $\vec{v}|_K \in \H^1(K)$ for each $K \in \meshT$  
	\item $\jump{\vec{v}\cdot\n} = 0$ for each $\mathcal{F} \in \Gamma_0$ 
\end{enumerate}
then $\vec{v} \in H(\div;\D)$. Conversely, if $\vec{v} \in H(\div;\D)$ and (a) is satisfied, then (b) holds. 
\end{prop}
\begin{proof}
It must be shown that, given (a) and (b), $\nabla\cdot\vec{v} \in L^2(\D)$. From (a), $\nablah\cdot\vec{v} \in L^2(\D)$. Using Green's identity and (b): 
	\begin{equation}
	\begin{aligned}
		\int u\nablah\cdot\vec{v} \ud \x &= \sum_{K\in\meshT} \bracket{\int_{\partial K} u \vec{v}\cdot\n \ud s - \int_K \nabla u \cdot \vec{v} \ud \x } \\
		&= \int_{\Gamma_0} u \jump{\vec{v}\cdot\n} \ud s - \int \nabla u \cdot \vec{v} \ud \x \\
		&= \int u\, \nabla\cdot\vec{v} \ud \x \,, 
	\end{aligned}
	\end{equation}
holds for $u$ sufficiently smooth and vanishing on the boundary (i.e.~$u \in C_0^\infty(\D)$). Thus, $\nabla\cdot\vec{v} = \nablah\cdot\vec{v} \in L^2(\D)$. 

On the other hand, if $\vec{v} \in H(\div;\D)$ then $\nabla\cdot\vec{v} = \nablah\cdot\vec{v}$ and, given $\vec{v}|_K \in \H^1(K)$, we obtain 
	\begin{equation}
		\int_{\Gamma_0} u \jump{\vec{v}\cdot\n} \ud s = 0 \,, \quad \forall u \in C_0^\infty(\D) \,, 
	\end{equation}
hence, (b) holds.
\end{proof}
In other words, a discrete subspace of $H(\div;\D)$ must (a) have a smooth function space on each element and (b) have suitable matching conditions so that the normal component is continuous across interior mesh interfaces. 
Figure \ref{fem:quiver} depicts an example vector in the space $H(\div;\D)$. The vector field is piecewise discontinuous on each element but since the normal component is the same in $K_1$ and $K_2$, this vector field is a member of $H(\div;\D)$. 
% --- RT quiver diagram --- 
\begin{figure}
	\centering
	\includegraphics[width=.5\textwidth]{figs/quiver.pdf}
	\caption{An example vector in $H(\div;\D)$. Note that the normal component is continuous across the shared face between the two elements while the tangential component is discontinuous. }
	\label{fem:quiver}
\end{figure}

In two spatial dimensions, the RT space uses the local polynomial space $\Qcal{p+1,p} \times \Qcal{p,p+1}$. This choice can be motivated by the discrete de Rham complex \cite{mfem_brezzi} in that 
	\begin{equation}
		\Qcal{p+1} \xrightarrow{\nabla\times} \Qcal{p+1,p}\times \Qcal{p,p+1} \xrightarrow{\nabla\cdot} \Qcal{p} \,. 
	\end{equation}
As an example, the lowest-order polynomial space is
	\begin{equation}
		\Qcal{1,0} \times \Qcal{0,1} = \spn\bracet{\twovec{1}{0}\,, \twovec{\xi}{0} \,, \twovec{0}{1} \,, \twovec{0}{\eta}} \,, 
	\end{equation}
and thus we have that: 
	\begin{equation}
		\hat{\nabla}\cdot \Qcal{1,0} \times \Qcal{0,1} = \spn\bracet{1} = \Qcal{0} \,. 
	\end{equation}
The nodal basis for $\Qcal{p+1,p} \times \Qcal{p,p+1}$ uses the closed Gauss-Lobatto points in the normal direction and the open Gauss-Legendre points in the tangential direction. The interpolating points for the first three orders are shown in Fig.~\ref{fem:rt_local_poly}. The circles denote degrees of freedom corresponding to the $\xi$ component while squares denote the $\eta$ component. In three dimensions, the local polynomial space is spanned by $\Qcal{p+1,p,p} \times \Qcal{p,p+1,p} \times \Qcal{p,p,p+1}$.%
% --- RT local polynomial space --- 
\begin{figure}
\centering
\begin{subfigure}{.25\textwidth}
	\centering
	\includegraphics[width=\textwidth]{figs/rt0.pdf}
	\caption{}
\end{subfigure}\qquad
\begin{subfigure}{.25\textwidth}
	\centering
	\includegraphics[width=\textwidth]{figs/rt1.pdf}
	\caption{}
\end{subfigure}\qquad
\begin{subfigure}{.25\textwidth}
	\centering
	\includegraphics[width=\textwidth]{figs/rt2.pdf}
	\caption{}
\end{subfigure}
\caption{The interpolating points used for the nodal basis of the space $\Qcal{p+1,p}\times \Qcal{p,p+1}$ for (a) $p=0$, (b) $p=1$, and (c) $p=2$. Gauss-Legendre points are used in the tangential direction and Gauss-Lobatto in the normal direction for each component of the vector. Circles denote the degrees of freedom associated with the $\xi$ component and squares the $\eta$ component. }
\label{fem:rt_local_poly}
\end{figure}

The contravariant Piola transformation is used to allow sharing the degrees of freedom associated with the normal component with neighboring elements. Combining the local function space $\Qcal{p+1,p}\times\Qcal{p,p+1}$ with the contravariant Piola transform yields: 
	\begin{equation}
		\mathbb{D}_p(K) = \{ \vec{v} = \frac{1}{J}\mat{F}\hat{\vec{v}}\circ\T^{-1} : \hat{\vec{v}} \in \Qcal{p+1,p} \times \Qcal{p,p+1} \} \,. 
	\end{equation}
Here, both the inverse mesh transformation and $1/J$ are generally non-polynomial when $\T$ is non-affine. 

We now define the degree-$p$ RT space as: 
	\begin{equation}
		\RT_p = \{ \vec{v} \in H(\div;\D) : \vec{v}|_K \in \mathbb{D}_p(K) \,, \quad \forall K \in \meshT \} \,. 
	\end{equation}
Note that since the contravariant Piola transform is used, functions in RT transform according to Eqs.~\ref{fem:piola}, \ref{fem:piola_grad}, and \ref{fem:piola_div}. 
The location of the degrees of freedom for $\RT_1$ are shown on a $3\times 3$ mesh in Fig.~\ref{fem:rtfes}. Continuity in the normal component is enforced by sharing the degrees of freedom corresponding to the normal component on interior faces. 
From Proposition \ref{fem:div_prop}, $\vec{v} \in \RT_p$ satisfies $\nabla\cdot\vec{v} = \nablah\cdot\vec{v} \in L^2(\D)$. However, the RT space does not have the continuity to allow a square-integrable gradient. In other words, $\nabla\vec{v} \notin [L^2(\D)]^{2\times 2}$ and $\nabla\vec{v} \neq \nablah\vec{v}$. 
% --- RT FES --- 
\begin{figure}
\centering 
\includegraphics[width=.3\textwidth]{figs/rtfes.pdf}
\caption{The distribution of degrees of freedom corresponding to the first degree Raviart Thomas space. Continuity of the normal component is enforced by sharing the degrees of freedom corresponding to the normal component along the interior face between neighboring elements. The circles and squares denote degrees of freedom in the $x$ and $y$ directions, respectively. }
\label{fem:rtfes}
\end{figure}

\subsection{Raviart Thomas Trace Space}
The normal trace of the RT space is required for the hybridization procedure discussed in Section \ref{rtvef_sec:hyb}. This space is defined on the interior skeleton of the mesh $\Gamma_0$ and represents the normal component of the RT space along the interior mesh faces. Let $\mathcal{P}_p$ be the space of univariate polynomials with degree at most $p$ and 
	\begin{equation}
		\mathbb{P}_p(\mathcal{F}) = \{ u = \hat{u} \circ \T^{-1} : \hat{u} \in \mathcal{P}_p(\hat{\mathcal{F}}) \}
	\end{equation}
the space of univariate polynomials mapped from the reference line, $\hat{\mathcal{F}} = [0,1]$. 
The RT trace space is then 
	\begin{equation}
		\Lambda_p = \{ \mu \in L^2(\Gamma_0) : \mu|_\mathcal{F} \in \mathbb{P}_p(\mathcal{F}) \,, \quad \forall \mathcal{F} \in \Gamma_0 \} \,. 
	\end{equation}
The degrees of freedom in $\Lambda_1$ are depicted in Fig.~\ref{fem:ifes}. Note that these degrees of freedom are exactly the degrees of freedom corresponding to the normal component of $\RT_1$ on the interior faces of the mesh. 
% --- RT trace space --- 
\begin{figure}
\centering
\includegraphics[width=.3\textwidth]{figs/ifes0.pdf}
\caption{The distribution of degrees of freedom corresponding to $\Lambda_1$, the space defined as the normal trace of the first degree Raviart Thomas space, on a $3\times 3$ mesh. }
\label{fem:ifes}
\end{figure}

\section{Computational Aspects}
In the previous section, we defined piecewise polynomial spaces that, through a clever choice of degrees of freedom, are finite-dimensional subspaces of the Sobolev spaces introduced in Section \ref{fem_sec:sobolev}. In this section, we discuss their computer implementation. A canonical basis with small support such that each basis function is non-zero on only a few elements in the mesh is built. We then discuss finite element assembly which leverages the small support of the basis functions to achieve an efficient algorithm for forming the matrices associated with a finite element space and bilinear or linear form. 

\subsection{The Canonical Basis}
Let $X_h$ denote one of the finite element spaces described in the previous section and $\mathcal{N}_h$ denote the set of nodes associated with $X_h$. For example, if $X_h = V_1$, the linear continuous finite element space, then $\mathcal{N}_h$ is the set of locations, $\x_i$, in the domain corresponding to a vertex in the mesh. A function $u_h \in X_h$ is determined by the values it takes at the nodes in $\mathcal{N}_h$. That is, $u_h \in X_h$ is determined by the set 
	\begin{equation}
		\Sigma_h = \{ u_h(\x_i)\,, \quad \forall \x_i \in \mathcal{N}_h\} \,, 
	\end{equation}
called the set of degrees of freedom for the space $X_h$. Defining functions $b_i$ that satisfy 
	\begin{equation}
		b_i \in X_h \,, \quad b_i(\x_j) = \delta_{ij}\,, 1 \leq i,j \leq \dim(X_h) \,, 
	\end{equation}
it is seen that $\{b_i\}$ forms a basis for $X_h$. The $b_i$ are referred to as basis functions. A selection of basis functions for $V_1$, $V_2$, and $V_3$ in one dimension are depicted in Fig.~\ref{fem:fe_support_1d}. Observe that each $b_i$ is one at $x_i$ and zero for each $x_j$ where $i\neq j$ and that each basis function has a localized support. That is, the basis functions associated with the boundary of the domain are non-zero only on a single element. This is also true for the ``bubble'' basis functions corresponding to a node that is on the interior of an element (e.g.~$b_8$ in the middle diagram). Basis functions associated with a node shared by multiple elements are non-zero only on those elements. This is demonstrated by $b_4$, $b_7$, and $b_{10}$ in the upper, middle, and lower diagrams, respectively. Figure \ref{fem:fe_support} shows a selection of basis functions for $V_1$ in two dimensions. Again, the basis functions are non-zero only in the elements that share a node. This property of local support was constructed by design to to keep the resulting algebraic system as computationally manageable and memory efficient as possible. 
% --- 1D support diagram --- 
\begin{figure}
\centering
\includegraphics[width=.85\textwidth]{figs/fe_support_1d.pdf}
\caption{A selection of the global basis functions for a linear (upper), quadratic (middle), and cubic (lower) continuous finite element space in one dimension. The basis functions are zero outside of their local support. }
\label{fem:fe_support_1d}
\end{figure}

% --- fe support diagram --- 
\begin{figure}
\centering
\includegraphics[width=.65\textwidth]{figs/fe_support.pdf}
\caption{A selection of global basis functions for a linear continuous finite element space in two dimensions. The basis functions are non-zero only the finite number of elements that share a node. }
\label{fem:fe_support}
\end{figure}

For DG spaces, degrees of freedom are not shared between elements. Thus, the global basis is spanned by the of collection of the element-local basis functions corresponding to each element in the mesh. In this case, the support of the basis functions is limited to a single element. 

Using this basis, a function $u \in X_h$ can be represented as 
	\begin{equation}
		u(\x) = \sum_i b_i(\x) u_i 
	\end{equation}
where $u_i \in \Sigma_h$ is the degree of freedom corresponding to $b_i$. 

\subsection{Finite Element Assembly}
Consider the abstract, finite-dimensional problem: find $u \in X_h$ such that 
	\begin{equation}
		a(v,u) = f(v) \,, \quad \forall v \in X_h \,. 
	\end{equation}
Both the test function $v$ and the solution $u$ are represented as a linear combination of the canonical basis functions $\{b_i\}$ that span $X_h$. That is, we write 
	\begin{equation}
		u = \sum_i b_i u_i \,, \quad v = \sum_i b_i v_i \,,
	\end{equation}
where $\{u_i\}$ and $\{v_i\}$ are the degrees of freedom that determine $u,v \in V$. Inserting this representation into the abstract problem yields 
	\begin{equation}
		a(\sum_i b_i v_i, \sum_j b_j u_j) = f(\sum_i b_i v_i) \iff \sum_i \sum_j v_i a(b_i,b_j) u_j = \sum_i v_i f(b_i) \,, 
	\end{equation} 
where we have used the bilinearity and linearity of $a$ and $f$, respectively. 
Letting $\mat{A}$ be the matrix with entries $a(b_i, b_j)$ and $\underline{f}$ the vector with entries $f(b_i)$, the above is equivalent to 
	\begin{equation}
		\underline{v}^T \mat{A} \underline{u} = \underline{v}^T \underline{f} \,, 
	\end{equation}
where $\underline{w}$ represents the vector of degrees of freedom corresponding to some $w \in X_h$. 
We wish to have this hold $\forall v \in X_h \Rightarrow \forall v_i$. Therefore, solving the abstract problem is equivalent to solving 
	\begin{equation}
		\mat{A} \underline{u} = \underline{f} \,, 
	\end{equation}
for the vector of degrees freedom $\underline{u}$. 

We now show that the algebraic system $\mat{A}\underline{u} = \underline{f}$ is sparse in that most of its entries are zero. This fact arises from the local support of the basis functions $b_i$. Let $\supp(b_i) \subset \D$ denote the subset of the domain where $b_i$ is non-zero. In other words, $\supp(b_i)$ is the union of adjacent elements that share the node located at $\x_i$. Then, $a(b_i, b_j)$ is non-zero only where $\supp(b_i)$ and $\supp(b_j)$ overlap i.e.~when $\supp(b_i) \cap \supp(b_j) \neq \emptyset$. Using the basis functions depicted in the upper diagram of Fig.~\ref{fem:fe_support_1d} as an example, the term $a(b_1,b_7)$ is zero since $b_7$ is zero where $b_1$ is non-zero and vice versa. On the other hand, $a(b_4,b_5)$ is non-zero since $b_4$ and $b_5$ are both non-zero in $K_4$. Figure \ref{fem:sparse1d} shows the resulting sparsity pattern corresponding to the finite element spaces depicted in Fig.~\ref{fem:fe_support_1d}. 
% --- 1D sparsity pattern --- 
\begin{figure}
\centering
\begin{subfigure}{.32\textwidth}
	\centering
	\includegraphics[width=\textwidth]{figs/sparse1d.pdf}
	\caption{}
\end{subfigure}
\begin{subfigure}{.32\textwidth}
	\centering
	\includegraphics[width=\textwidth]{figs/sparse1d_1.pdf}
	\caption{}
\end{subfigure}
\begin{subfigure}{.32\textwidth}
	\centering
	\includegraphics[width=\textwidth]{figs/sparse1d_2.pdf}
	\caption{}
\end{subfigure}
\caption{Sparsity patterns associated with the finite element spaces depicted in Fig.~\ref{fem:fe_support_1d}. }
\label{fem:sparse1d}
\end{figure}

In this document, we will frequently build the matrix, $\mat{A}$, corresponding to a bilinear form, $a(\cdot,\cdot)$, on the space $X_h$ using the following notation: 
	\begin{equation}
		\underline{v}^T \mat{A} \underline{u} = a(v,u) \,, \quad u,v \in X_h \,. 
	\end{equation}
In the above, we implicitly write $u = \sum u_i b_i$ and $v = \sum v_i b_i$ to form the matrix $\mat{A}_{ij} = a(b_i,b_j)$. Analogously, we build the vector $\underline{f}$ corresponding to the linear form $f(\cdot)$ using  
	\begin{equation}
		\underline{v}^T \underline{f} = f(v) \,, 
	\end{equation}
where $v = \sum v_i b_i$ is implicitly used to form $\underline{f}_i = f(b_i)$. 

\section{Iterative Solution Methods for Linear Systems}
We consider the algebraic system 
	\begin{equation}
		\mat{A} \underline{x} = \underline{b} \,, 
	\end{equation}
where $\mat{A}$ is a non-singular matrix of dimension $n$. Most iterative methods are based on a suitable splitting of $\mat{A}$ such that 
	\begin{equation}
		\mat{A} = \mat{P} - \mat{N} \,, 
	\end{equation}
where $\mat{P}$ is non-singular and ideally computationally efficient to invert. The iteration proceeds as 
	\begin{equation}
		\mat{P} \underline{x}^{k+1} = \mat{N}\underline{x}^k + \underline{b} \,,
	\end{equation}
where the superscript denotes iteration index with $\underline{x}^0$ an initial guess. 
Such an iteration converges if the spectral radius of the iteration matrix $\mat{P}^{-1}\mat{N}$ is less than unity. By adding and subtracting $\mat{A}\underline{x}^k$ on the right hand side, this iteration is equivalent to 
	\begin{equation}
		\mat{P}\underline{x}^{k+1} = (\mat{N} + \mat{A})\underline{x}^k + \underline{b} - \mat{A}\underline{x}^k = \mat{P}\underline{x}^k + \underline{r}^k \,, 
	\end{equation}
where the residual vector is $\underline{r}^k = \underline{b} - \mat{A}\underline{x}^k$. Operating by $\mat{P}^{-1}$ yields the iteration: 
	\begin{equation}
		\underline{x}^{k+1} = \underline{x}^k + \mat{P}^{-1} \underline{r}^k \,. 
	\end{equation}

\subsection{Classical Iterative Schemes}
Let $\mat{A} = \mat{D} + \mat{L} + \mat{U}$ where $\mat{D}$, $\mat{L}$, and $\mat{U}$ are the diagonal, strictly lower triangular, and strictly upper triangular parts of the matrix $\mat{A}$. That is, 
	\begin{equation}
		\mat{D}_{ij} = \begin{cases}
			\mat{A}_{ij} \,, & i=j \\ 
			0 \,, & \text{otherwise}
		\end{cases} \,, \quad 
		\mat{L}_{ij} = \begin{cases}
			\mat{A}_{ij} \,, & i>j \\ 
			0\,, & \text{otherwise}
		\end{cases} \,, \quad 
		\mat{U} = \begin{cases}
			\mat{A}_{ij} \,, & i<j\\
			0\,, & \text{otherwise}
		\end{cases} \,. 
	\end{equation}
The Jacobi method sets $\mat{P} = \mat{D}$ and $\mat{N} = \mat{L} + \mat{U}$ so that 
	\begin{equation}
		\underline{x}^{k+1} = \underline{x}_k + \mat{D}^{-1}\underline{r}^k \,. 
	\end{equation}
Since $\mat{D}$ is a diagonal matrix, the action $\mat{D}^{-1}\underline{r}^k$ can be applied cheaply by dividing the entries of $\underline{r}$ by the diagonal entries of $\mat{A}$. 

The Gauss-Seidel method uses the splitting $\mat{P} = \mat{D} + \mat{L}$ and $\mat{N} = \mat{U}$. The iteration is then 
	\begin{equation}
		\underline{x}^{k+1} = \underline{x}^k + \paren{\mat{D} + \mat{L}}^{-1} \underline{r}^k \,. 
	\end{equation}
Note that since the matrix $\mat{D} + \mat{L}$ is lower triangular, the action of its inverse can be applied using forward substitution. Forward substitution requires more floating point operations than inverting a diagonal matrix. However, this added work allows Gauss-Seidel to typically converge faster than Jacobi. 

\subsection{The Conjugate Gradient Method}
For symmetric positive definite matrices, the solution $\mat{A}\underline{x} = \underline{b}$ is the unique minimizer of the potential 
	\begin{equation}
		\Pi = \frac{1}{2} \underline{x}^T \mat{A} \underline{x} - \underline{x}^T \underline{b} \,. 
	\end{equation}
This can be seen by setting the gradient of $\Pi$ to zero: 
	\begin{equation}
		0 = \nabla \Pi = \mat{A} \underline{x} - \underline{b} \iff \mat{A} \underline{x} = \underline{b} \,. 
	\end{equation}
The conjugate gradient method is built around the idea of minimizing $\Pi$ in such a way that the system is guaranteed to be solved in at most $N$ steps, where $N$ is the number of rows or columns in the square matrix $\mat{A}$. 

Note that the residual $\underline{r}^k = \underline{b} - \mat{A}\underline{x}^k = -\nabla\Pi$. Moving in the direction of the residual then moves opposite the gradient of the potential. The method of steepest descent is the method that updates the solution in the direction of the residual in such a way that the potential is minimized. That is, we seek to use the update $\underline{x}^{k+1} = \underline{x}^k + \lambda_k \underline{r}^k$ where $\lambda_i \in \R$ is chosen so that $\underline{x}^{k+1}$ minimizes the potential. Introducing this update into the potential: 
	\begin{equation}
		\Pi = \frac{1}{2}\paren{\underline{x}^k + \lambda_k \underline{r}^k}^T \mat{A}\paren{\underline{x}^k + \lambda_k \underline{r}^k} - \paren{\underline{x}^k + \lambda_k \underline{r}^k}^T \underline{r}^k \,. 
	\end{equation}
Setting $\pderiv{\Pi}{\lambda_k} = 0$ and solving for $\lambda_k$ yields: 
	\begin{equation}
		\lambda_k = \frac{\underline{r}^k \cdot \underline{r}^k}{\underline{r}^k \cdot \mat{A} \underline{r}^k} \,. 
	\end{equation}

The conjugate gradient is a type of steepest descent method where the search directions are chosen to be $\mat{A}$-orthogonal. In this way, the algorithm only searches in new directions and avoids stalls associated with exploring parts of the solution space that have already been explored. In other words, both the parameter $\lambda_k$ in the update for $\underline{x}^{k+1}$ and the search direction are chosen optimally. The update used by conjugate gradient is 
	\begin{equation}
		\underline{x}^{k+1} = \underline{x}^k + \lambda_k \underline{z}^k \,, 
	\end{equation}
where the search direction, $\underline{z}^k$, is chosen such that 
	\begin{equation}
		\underline{z}^k = \underline{r}^k + \theta_k \underline{z}^{k-1} \,. 
	\end{equation}
Here, $\theta_k$ is the parameter that forces $\underline{z}^{k}$ and $\underline{z}^{k-1}$ to be $\mat{A}$-conjugate such that $\underline{z}^k \cdot \mat{A} \underline{z}^{k-1} = 0$: 
	\begin{equation}
		0 = \underline{z}^k \cdot \mat{A} \underline{z}^{k-1} \Rightarrow \theta_k = -\frac{\underline{r}^k\cdot \mat{A} \underline{z}^{k-1}}{\underline{z}^{k-1} \cdot \mat{A} \underline{z}^{k-1}} \,. 
	\end{equation}
With this update, the $\lambda_k$ that minimizes the potential is 
	\begin{equation}
		\lambda_k = \frac{\underline{z}^k \cdot \underline{r}^k}{\underline{z}^k \cdot \mat{A} \underline{z}^k} \,. 
	\end{equation}
The solution procedure for a 2-dimensional system is shown in Fig.~\ref{fem:conj_grad_diag}. The search directions are chosen such that they are $\mat{A}$-orthogonal allowing the algorithm to find the solution in two steps. 
% --- conjugate gradient on 2-dimensional diffusion problem --- 
\begin{figure}
\centering
\includegraphics[width=.65\textwidth]{figs/conj_grad.pdf}
\caption{A depiction of the conjugate gradient solution process for solving a two-dimension symmetric positive definite system. The solution iterates are shown as dots and the search directions as arrows. The potential is minimized in two iterations.}
\label{fem:conj_grad_diag}
\end{figure}

Note that the solution satisfying $\min \Pi$ is only true when $\mat{A}$ is symmetric positive definite. Furthermore, when $\mat{A}$ is not symmetric positive definite, $\underline{x}\cdot\mat{A}\underline{x}$ is not an inner product. This means $\mat{A}$-orthogonality cannot be built using the three-term recursion process outlined above and that the previous search directions would need to be stored in order to enforce $\mat{A}$-orthogonality. 

% \subsection{The Stabilized Bi-Conjugate Gradient Method}
% In this document, \gls{bicg} \cite{doi:10.1137/0913035} is used to solve non-symmetric systems where the conjugate gradient method would not converge. \gls{bicg} is also a steepest descent method that seeks to find the search directions in an optimal manner. For \gls{bicg}, bi-conjugate search directions are built through repeated applications of the matrix and its transpose, respectively. 

% \subsection{Multigrid}
% \cite{multigrid_tutorial}
\end{document}