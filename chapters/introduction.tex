\documentclass[../doc.tex]{subfiles}

\begin{document}
\chapter{Introduction}
The interaction of radiation with matter, referred to as radiation transport, is an important physical process in many fields including nuclear engineering, medical physics, and \gls{hedp}. Precise understanding of the distribution of radiation in a system is crucial to designing efficient nuclear reactors, safe cancer treatments, and ground breaking \gls{hedp} experiments. Once obtained, the radiation distribution can be used to compute the power level of a nuclear reactor core, the administered dose of a cancer treatment, or the radiative energy deposition in an \gls{hedp} experiment. In all these fields, physical experiments are often prohibitively expensive, subject to strict regulations, and dangerous to humans. Predictive modeling avoids these issues by leveraging approximate computational models that can be used to gain insight and test ideas without physical experimentation. These models enable scientists to design more productive experiments and discover more with fewer resources. However, as scientists demand increasingly predictive models, the design and implementation of the computational models themselves have become significant topics of research. The goal of this research is to create software that produces a more accurate answer for less computational work. Users of such software can then tackle larger problems or perform more design iterations for the same cost in runtime and ultimately the same electricity bill. 

Computational models of radiation transport are generally categorized into stochastic (Monte Carlo), deterministic, or hybrid methods which combine stochastic and deterministic. All methods seek an approximate solution to the Boltzmann Transport Equation (BTE), the 7-dimensional integro-differential equation that mathematically describes radiation transport. In this dissertation, we focus on deterministic methods and in particular those that employ the Discrete Ordinates ($S_n$) angular model. Despite the known drawbacks of ray effects and discrete frequency groups, $S_n$ methods are popular in \gls{hedp} simulations as they are efficient enough that it is computationally tractable to fully converge the nonlinear temperature dependence of the material energy balance equation. While stochastic methods such as implicit Monte Carlo (IMC) have undoubtedly better angular and frequency resolution (when stochastic variance is sufficiently reduced), it is typically infeasible to perform more than one nonlinear temperature iteration per time step. Thus, in \gls{hedp} simulations, $S_n$ methods are both faster and more accurate. 

$S_n$ methods realize this efficiency by using iterative methods that decouple the spatial, angular, and frequency dependence. At each iteration, only the solution of a spatially-dependent system of equations is required. Furthermore, the existence of the extremely optimized and mature matrix-free inversion technique known as the ``transport sweep'' enables one to only solve a small grouping of unknowns at a time. This allows the 7-dimensional $S_n$ equations to be solved with a comparable memory footprint to that of the 4-dimensional hydrodynamics equations. In the context of \gls{hedp} simulations where it is difficult to store even the transport \emph{solution vector} due to its size, this is the difference between being able to run ``full'' transport calculations and having to use approximate angular models such as \gls{fld} due to memory restrictions alone. 

Unfortunately, the performance of these iterative methods degrade in optically thick and diffusive media. Many problems of practical interest including \gls{icf} capsules and nuclear reactor cores contain diffusive regions that cause the iterative efficiency to stall. Many iterative preconditioners have been devised to robustly accelerate convergence independent of material properties. The most popular method is diffusion synthetic acceleration (DSA), a two-level in angle multigrid preconditioner that uses the diffusion approximation as the ``coarse'' level. It is well known that DSA is unstable in the thick diffusion limit unless the transport and diffusion equations are discretized in a consistent manner \cite{A,LarsenDSA}. The development of consistent \cite{WWM} and ``partially consistent'' \cite{ML,AM,WR} DSA methods has been the topic of significant research. Modifications to traditional transport methods, such as the addition of negative flux fixups or the use of exotic transport discretizations, require one to carry these modifications through the derivation of the DSA method in order to have numerical stability in the thick diffusion limit; a non-trivial task that significantly raises the research burden of developing new methods. 

The \gls{vef} method \cite{mihalas}, also known as Quasidiffusion \cite{goldin}, is a rapidly-converging nonlinear scheme for solving the radiation transport equation. The \gls{vef} method uses transport-dependent, nonlinear closures of the moment or $P_1$ equations to create a weakly nonlinear problem that converges rapidly even with fixed-point iteration. In contrast to DSA, \gls{vef} methods enable significant algorithmic flexibility including the use of so-called ``independent'' discretizations \cite{two-level-independent-warsa,me,olivier_mandc} and negative flux fixups \cite{YEE2020109696} while still retaining the thick diffusion limit. Independent \gls{vef} methods use discretizations of the \gls{vef} equations that are not asymptotically consistent with the transport discretization\footnote{note asymptotic consistency refers to the transport concept of numerical consistency in the asymptotic thick diffusion limit not the mathematical notion of consistency e.g. as the mesh is refined.}. Thus, two solutions are produced, one from transport and one from \gls{vef}. These two solutions differ on the order of the discretization error and are only equivalent in the limit of spatial mesh refinement. While it is possible to formulate ``consistent'' \gls{vef} methods where the transport and \gls{vef} solutions are equivalent on all meshes, \citeauthor{two-level-independent-warsa} have shown that consistent methods have the same iterative properties as independent methods without the flexibility that independent methods allow \cite{two-level-independent-warsa}. Previous work has primarily focused on using finite volume \cite{weasel,wieselquist,Jones2019TheQM} or lowest-order finite element methods \cite{two-level-independent-warsa,ho_trt_maginot,LOU2019258} with the latter being limited to one dimension only. 

In practical calculations, the solution of the discretized \gls{vef} equations is wrapped in possibly multiple outer iterations corresponding to time integration, nonlinear thermal radiative transfer (TRT) iterations, and/or linear source iterations. A fast iterative solver is then crucial to the overall efficiency of the transport algorithm. In fact, without an efficient linear solver \gls{vef} methods cannot compete against traditional methods such as DSA. The \gls{vef} equations are particularly troublesome for iterative methods due to their inherent lack of symmetry arising from the Eddington tensor closure terms. In addition, the low regularity of the \gls{vef} data leads to uncommon and oftentimes unnatural discretization techniques that make leveraging existing solver technology difficult. Previous works either leave efficient solvers as future work \cite{two-level-independent-warsa,me,LOU2019258} or rely on expensive and un-scalable preconditioners such as incomplete LU (ILU) factorization \cite{weasel}. 

Tangentially, recent trends in computer architecture, namely the ending of Moore's Law\footnote{the empirical observation that the number of transistors in an integrated circuit doubles every two years.}, indicate computers will be increasingly parallel and dependent on domain specific architectures such as \glspl{gpu}. This is especially evident in the Top500 list\footnote{a ranked list of the world's 500 most powerful supercomputers.}: from June 2010 to June 2020, the average number of CPU cores per socket rose from 4 to 20. In that same time span, the number of GPU-accelerated supercomputers increased from none to 134 (citation needed, dates likely wrong). Furthermore, it has been observed that floating point throughput is improving faster than memory latency and bandwidth. Thus, data movement will become increasingly expensive relative to computation. For GPU-accelerated computers, data movement is further compounded by the need to transfer data to and from the GPU. 

In light of these trends, the Department of Energy's Center for Efficient Exascale Discretiations (CEED) within the Exascale Computing Project (ECP) has targeted high-order finite element methods as one of it's main research thrusts. Compared to low-order methods, high-order methods are more accurate for the same number of unknowns (on smooth problems) and have better data reuse and locality. In other words, high-order methods have a higher floating point to memory access ratio\footnote{this ratio is called the \emph{arithmetic intensity}.} that makes them more amenable to efficient implementation on emerging high performance computer (HPC) architectures. In hydrodynamics simulations, high-order methods that use high-order (curved) meshes have been shown to provide greater robustness (especially in the presence of significant mesh distortions), symmetry preservation, and excellent strong scaling \cite{blast,blast2,blast3}. In tightly coupled multiphysics calculations, such as in \gls{hedp} simulations, robustness and accuracy are typically limited by the least robust and least accurate component. Because of this, high-order methods and curved meshes are also desired for radiation transport. While no such claims have been verified for radiation transport specifically, high-order transport methods on curved meshes are under active development \cite{graph_sweeps,ldrd_dsa,YEE2020109696,olivier_mandc}. 

The goal of this dissertation is to develop efficient, high-order finite element, independent \gls{vef}-based transport methods that are compatible with curved meshes and readily solvable with existing linear solver techniques. The extension of the \gls{vef} method to multi-dimensional, high-order finite element discretizations and the development of fast and scalable linear solvers are the primary contributions of this dissertation. The remainder of this manuscript proceeds as follows: 
	\begin{itemize}
		\item Chapter 2 discusses the Boltzmann transport equation, the derivation of the \gls{vef} equations and their boundary conditions, and the existing high-order finite element methods used to discretize the $S_n$ equations. 
		\item Chapter 3 presents two one-dimensional high-order mixed finite element (MFEM) discretizations of the \gls{vef} equations, post processing techniques to increase their accuracy, and the corresponding efficient linear solvers. 
		\item Chapter 4 extends the one-dimensional discretizations and their solvers to multiple dimensions where possible.
		\item Chapter 5 presents two alternate \gls{dg}-based \gls{vef} methods that leverage recently developed linear solver techniques developed in \cite{pazner_usc}. 
		\item Chapter 6 compares the three \gls{vef} methods on proxy problems of interest from thermal radiative transfer.
		\item Finally, Chapter 7 summarizes the results of this dissertation and discusses the remaining future work. 
	\end{itemize}

% Historically, Moore's law\footnote{Moore's law is an empirical correlation not a physical law but Moore's empirical correlation doesn't sound quite as nice.} predicts that a computer bought one year from now will have twice as many integrated circuits as one bought today. Scientists had the choice to divert valuable research time to optimizing code or simply wait a few years to buy a faster computer to run their code on. 

% However, computer architects are quickly approaching the atomic limits of miniaturizing circuits; one can no longer expect computing power to double every year. To produce analogous increases in computational power, architects have turned toward parallelism. While a single processor can no longer double in speed in one year, doubling the computational power could be achieved by doubling the number of processors. In practice, these additional processors often share resources leading to operational conflicts. The classic analogy is: two computational radiation transport researchers trying to check out the campus library's single copy of Lewis and Miller's 1984 classic ``Computational Methods of Neutron Transport'' for the literature review of their papers. Only one of the researchers can have the book at a time meaning the other must remain idle (to their advisors chagrin). Minimizing the impact of conflicts such as these requires both careful programming and clever algorithmic design. For example, the two researchers could reorder their work such that one reads Lewis and Miller while the other writes the results section of their paper. 

% Additionally, computer designers are increasingly turning toward domain specific architectures designed through hardware/software co-design. In co-design, hardware and software engineers work together to design a processor that can perform a specific task as fast as possible. For Apple, this was the difference between MacBook encryption incurring a huge performance penalty when encryption was performed in software and executed on the CPU to being the default setting once encryption was switched to a dedicated encryption processor. And in computer games, graphics processing units (GPUs) can process millions of pixels simultaneously enabling higher frame rates and resolutions than if a CPU alone was used. In both of these cases, there is a tradeoff between the ease of programming a general purpose CPU and the performance of utilizing a domain specific architecture. This tradeoff is especially severe in scientific computing where its small market share means the co-design process is rather one-sided. 

% In fact as of June 2020, the average number of cores per socket of the 500 most powerful computers in the world is 20 when it was only 4 10 years ago. Similarly, there are now 144 GPU accelerated on the Top500 list when there were zero 10 years ago (citation needed). These trends indicate the algorithmic and implementation burdens of parallelism and domain specific architectures will continue to increase. It will thus be increasingly important to design algorithms that leverage knowledge of the physics of the problem, the mathematics of numerical techniques, and computer architecture. 

\end{document}